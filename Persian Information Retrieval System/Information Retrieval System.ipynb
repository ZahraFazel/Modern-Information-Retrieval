{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MIRProjectPhase1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTWsrYobJnd3",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=30>\n",
        "<p></p><p></p>\n",
        "به نام خدا\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<img src=\"Images/sharif.png\" width=\"25%\">\n",
        "<font color=blue>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر سلیمانی\n",
        "</font>\n",
        "<p></p>\n",
        "<font color=green>\n",
        "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
        "</font>\n",
        "<p></p>\n",
        "<font color=#FF7500>\n",
        "بهار ۹۹\n",
        "<br>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXCFFyc8Jnd7",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center>مقدمه </div>\n",
        "</font>\n",
        "<hr>\n",
        "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
        "<br>\n",
        "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
        "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
        "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
        "ارزیابی کنید.\n",
        "<br>\n",
        "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
        "import\n",
        "مناسب از کد‌هایتان استفاده کنید.\n",
        "<br>\n",
        "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
        "</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-nJxAxdJnd8",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify\" dir=rtl><font face=\"XB Zar\" size=5 >\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center>مجموعه دادگان</div>\n",
        "</font>\n",
        "<hr>\n",
        "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
        "این مجموعه اسناد از دو بخش تشکیل شده است\n",
        ".\n",
        "<br>\n",
        "بخش اول که در فایل \n",
        "Persian.xml\n",
        "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
        "هر سند شامل شناسه\n",
        "(id)،\n",
        "عنوان\n",
        "(title)،   \n",
        "و متن \n",
        "(text)\n",
        "است.\n",
        "بخش دوم که در پوشه‌ی \n",
        "queries\n",
        "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
        "بخش سوم که در پوشه‌ی\n",
        "relevance\n",
        "آمده‌است،\n",
        "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
        "</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avT4ky8EJnd-",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl ><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
        "</font>\n",
        "<hr>\n",
        "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
        "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
        "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
        "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
        "مراجعه کنید.\n",
        "<br>\n",
        "<br>\n",
        "عملیات مورد انتظار:\n",
        "<ol>\n",
        "    <li>\n",
        "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
        "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
        "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
        "    </li>\n",
        "    <li> \n",
        "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
        "    </li>\n",
        "    <li>\n",
        "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
        "    </li>\n",
        "<li>\n",
        "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
        "ریشه\n",
        "(stemming)\n",
        "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
        "    </li>\n",
        "</ol>\n",
        "</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzEhdYtzJnd_",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
        "prepare_text\n",
        "باید طوری بر روی متن ورودی با نام\n",
        "raw_text\n",
        "عمل کند که\n",
        "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ98SbQMJneB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install hazm\n",
        "# !pip install parsivar\n",
        "from xml.dom.minidom import *\n",
        "from hazm import Normalizer, word_tokenize\n",
        "from parsivar import FindStems\n",
        "from string import punctuation\n",
        "from json import dump, load\n",
        "from copy import deepcopy\n",
        "from numpy.random import choice\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from re import findall\n",
        "from glob import glob\n",
        "from math import log10, log2\n",
        "from heapq import heappop, heappush, heapify\n",
        "\n",
        "\n",
        "validation = {}\n",
        "number_of_docs = 0\n",
        "positional_index = {}\n",
        "bigram_index = {}\n",
        "\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, doc_id, text_tokens, title_tokens):\n",
        "        self.doc_id = doc_id\n",
        "        self.text_tokens = text_tokens\n",
        "        self.title_tokens = title_tokens\n",
        "\n",
        "\n",
        "def read_page(page):\n",
        "    doc_id, title, text = None, None, None\n",
        "    for child in page.childNodes:\n",
        "        if child.localName == 'id':\n",
        "            doc_id = child.firstChild\n",
        "        if child.localName == 'title':\n",
        "            title = child.firstChild\n",
        "        if child.localName == 'revision':\n",
        "            for child_ in child.childNodes:\n",
        "                if child_.localName == 'text':\n",
        "                    text = child_.firstChild\n",
        "    return doc_id.data, title.data, text.data\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    normalizer = Normalizer()\n",
        "    stemmer = FindStems()\n",
        "    text = normalizer.normalize(text)\n",
        "    translate_table = dict((ord(char), None) for char in punctuation.__add__('»').__add__('«'))\n",
        "    text = text.translate(translate_table)\n",
        "    temp_tokens = word_tokenize(text)\n",
        "    text_tokens = [stemmer.convert_to_stem(token) for token in temp_tokens]\n",
        "    return text_tokens\n",
        "  \n",
        "# input : a DOM Element which its tag name is \"page\"\n",
        "# Example:\n",
        "# doc = parse(file_path)\n",
        "# pages = doc.getElementsByTagName('page')\n",
        "# for page in pages:\n",
        "#     document = prepare_text(page)\n",
        "\n",
        "def prepare_text(raw_text):\n",
        "    doc_id, title, text = read_page(raw_text)\n",
        "    text_tokens = preprocess(text)\n",
        "    title_tokens = preprocess(title)\n",
        "    return Document(doc_id, text_tokens, title_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "mlwmeLaQJneG",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
        "</font>\n",
        "<hr>\n",
        "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
        "<br>\n",
        "شرح نمایه‌های مورد انتظار:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
        "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
        "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
        "</li>\n",
        "<li>\n",
        "(*امتیازی*)\n",
        "نمایه‌ی \n",
        "Bigram: \n",
        "با استفاده از این نمایه می‌توان با دادن یک \n",
        "Bigram\n",
        "(ترکیب‌های دو حرفی) \n",
        "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
        "</li>\n",
        "</ol>\n",
        "</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjpoyRv9JneH",
        "colab_type": "text"
      },
      "source": [
        " <div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        " (30 نمره)\n",
        "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
        "construct_indexes\n",
        "با گرفتن مسیر مجموعه‌داده‌ها\n",
        "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_akK-pvJneI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bigrams(text):\n",
        "    new_text = \"$\" + text + \"$\"\n",
        "    bigrams = []\n",
        "    for i in range(len(new_text) - 1):\n",
        "        bigrams.append(new_text[i:i + 2])\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def construct_positional_indexes(docs_path):\n",
        "    doc = parse(docs_path)\n",
        "    pages = doc.getElementsByTagName('page')\n",
        "    auxiliary_positional_index = {}\n",
        "    auxiliary_bigram_index = {}\n",
        "    for page in pages:\n",
        "        document = prepare_text(page)\n",
        "        # text\n",
        "        for i in range(len(document.text_tokens)):\n",
        "            # positional index\n",
        "            if document.text_tokens[i] not in auxiliary_positional_index.keys():\n",
        "                auxiliary_positional_index[document.text_tokens[i]] = {}\n",
        "            if document.doc_id not in auxiliary_positional_index[document.text_tokens[i]].keys():\n",
        "                auxiliary_positional_index[document.text_tokens[i]][document.doc_id] = {}\n",
        "            if 'text' not in auxiliary_positional_index[document.text_tokens[i]][document.doc_id]:\n",
        "                auxiliary_positional_index[document.text_tokens[i]][document.doc_id]['text'] = []\n",
        "            auxiliary_positional_index[document.text_tokens[i]][document.doc_id]['text'].append(i)\n",
        "            # bigrams\n",
        "            text_bigrams = get_bigrams(document.text_tokens[i])\n",
        "            for bigram in text_bigrams:\n",
        "                if bigram not in auxiliary_bigram_index.keys():\n",
        "                    auxiliary_bigram_index[bigram] = []\n",
        "                if document.text_tokens[i] not in auxiliary_bigram_index[bigram]:\n",
        "                    auxiliary_bigram_index[bigram].append(document.text_tokens[i])\n",
        "        # title\n",
        "        for i in range(len(document.title_tokens)):\n",
        "            # positional index\n",
        "            if document.title_tokens[i] not in auxiliary_positional_index.keys():\n",
        "                auxiliary_positional_index[document.title_tokens[i]] = {}\n",
        "            if document.doc_id not in auxiliary_positional_index[document.title_tokens[i]].keys():\n",
        "                auxiliary_positional_index[document.title_tokens[i]][document.doc_id] = {}\n",
        "            if 'title' not in auxiliary_positional_index[document.title_tokens[i]][document.doc_id]:\n",
        "                auxiliary_positional_index[document.title_tokens[i]][document.doc_id]['title'] = []\n",
        "            auxiliary_positional_index[document.title_tokens[i]][document.doc_id]['title'].append(i)\n",
        "            # bigrams\n",
        "            title_bigrams = get_bigrams(document.title_tokens[i])\n",
        "            for bigram in title_bigrams:\n",
        "                if bigram not in auxiliary_bigram_index.keys():\n",
        "                    auxiliary_bigram_index[bigram] = []\n",
        "                if document.title_tokens[i] not in auxiliary_bigram_index[bigram]:\n",
        "                    auxiliary_bigram_index[bigram].append(document.title_tokens[i])\n",
        "        validation[document.doc_id] = True\n",
        "        global number_of_docs\n",
        "        number_of_docs += 1\n",
        "    return auxiliary_positional_index, auxiliary_bigram_index\n",
        "\n",
        "\n",
        "positional_index, bigram_index = construct_positional_indexes('data/Persian.xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51VMBVg3JneM",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش برای مشاهده \n",
        "posting list\n",
        "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
        "get_posting_list\n",
        "با گرفتن\n",
        "word\n",
        "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
        "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
        "title\n",
        "و\n",
        "text\n",
        "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLnvKnQ0JneN",
        "colab_type": "code",
        "outputId": "42f7aa7b-306b-4d94-c61f-84f6ad4945ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "def get_posting_list(word):\n",
        "    posting_list = deepcopy(positional_index[word] if word in positional_index.keys() else {})\n",
        "    for doc_id in posting_list.keys():\n",
        "        if not validation[doc_id]:\n",
        "            posting_list[doc_id] = None\n",
        "    posting_list = {k: posting_list[k] for k in posting_list.keys() if posting_list[k] is not None}\n",
        "    return posting_list\n",
        "\n",
        "\n",
        "get_posting_list('سلام')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'3199': {'text': [4857, 4897]},\n",
              " '3760': {'text': [2816]},\n",
              " '3894': {'text': [3239, 3241]},\n",
              " '4254': {'text': [5980, 6114]},\n",
              " '4391': {'text': [1385]},\n",
              " '4394': {'text': [3268, 3273]},\n",
              " '4423': {'text': [643]},\n",
              " '4718': {'text': [303, 1636, 2413, 2592, 3282, 5042]},\n",
              " '4766': {'text': [326]},\n",
              " '4824': {'text': [678]},\n",
              " '5299': {'text': [1145]},\n",
              " '5346': {'text': [551]},\n",
              " '5354': {'text': [514]},\n",
              " '5357': {'text': [516]},\n",
              " '5359': {'text': [483]},\n",
              " '5360': {'text': [320, 523]},\n",
              " '5364': {'text': [291, 483]},\n",
              " '5366': {'text': [755]},\n",
              " '5369': {'text': [269, 448]},\n",
              " '5370': {'text': [555]},\n",
              " '5371': {'text': [333, 417]},\n",
              " '5380': {'text': [808]},\n",
              " '5382': {'text': [196, 482]},\n",
              " '5486': {'text': [361, 599, 981, 2051, 2309, 2390, 2651, 2705]},\n",
              " '5499': {'text': [682]},\n",
              " '5509': {'text': [6723]},\n",
              " '5512': {'text': [786]},\n",
              " '5619': {'text': [7032]},\n",
              " '5729': {'text': [437]},\n",
              " '5985': {'text': [618]},\n",
              " '6035': {'text': [451]},\n",
              " '6183': {'text': [205, 792]},\n",
              " '6339': {'text': [875]},\n",
              " '6461': {'text': [182]},\n",
              " '6482': {'text': [464]},\n",
              " '6640': {'text': [469, 474]},\n",
              " '6749': {'text': [2418]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__qCYhAsJneS",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "(*امتیازی*)\n",
        "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
        "get_words_with_bigram\n",
        "یک ورودی به عنوان\n",
        "bigram\n",
        "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6qxX9KAJneT",
        "colab_type": "code",
        "outputId": "028d40aa-251b-410b-f9c9-6765845411e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def get_words_with_bigram(bigram):\n",
        "    return bigram_index[bigram] if bigram in bigram_index.keys() else []\n",
        "\n",
        "\n",
        "get_words_with_bigram('لا')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['میانگین\\u200cبارش\\u200cسالانه',\n",
              " 'تلاقی',\n",
              " 'ساوجبلاغ',\n",
              " 'سابلاغ',\n",
              " 'آلود&آلا',\n",
              " 'ایالات',\n",
              " 'ولایات',\n",
              " 'اعلام',\n",
              " 'اسقلال',\n",
              " 'میلاد',\n",
              " 'دلالت',\n",
              " 'مقالات',\n",
              " 'لازم',\n",
              " 'خلاصهٔ',\n",
              " 'علاقه',\n",
              " 'قبلا',\n",
              " 'علاقهٔ',\n",
              " 'علاوه',\n",
              " 'اطلاعاتی',\n",
              " 'اطلاعات',\n",
              " 'خلاصه',\n",
              " 'علامت',\n",
              " 'بالای',\n",
              " 'طولانی\\u200cمدت',\n",
              " 'علاقه\\u200cمند',\n",
              " 'الغزلان',\n",
              " 'اولاد',\n",
              " 'میلادی',\n",
              " 'استقلال',\n",
              " 'اسلامی',\n",
              " 'هلال',\n",
              " 'انقلاب',\n",
              " 'ولایة',\n",
              " 'سنگلاخی',\n",
              " 'الان',\n",
              " 'اسلام',\n",
              " 'بالاترین',\n",
              " 'الاستعمار',\n",
              " 'خلاف',\n",
              " 'الاستعماریه',\n",
              " 'زینت\\u200cآلات',\n",
              " 'سالانه',\n",
              " 'التحصیلان',\n",
              " 'بالاتر',\n",
              " 'الانوار',\n",
              " 'سطح\\u200cبالای',\n",
              " 'مردم\\u200cسالاری',\n",
              " 'کاملا',\n",
              " 'خلافت',\n",
              " 'الاسلام',\n",
              " 'اسلام\\u200cگرا',\n",
              " 'معمولا',\n",
              " 'انقلابی',\n",
              " 'لاکربی',\n",
              " 'الاخضرحزام',\n",
              " 'الاخضر',\n",
              " 'جبل\\u200cالاخضر',\n",
              " 'مسلاته',\n",
              " 'جبل\\u200cالاخضرجبل\\u200cالاخضر',\n",
              " 'دلار',\n",
              " 'محصولات',\n",
              " 'ماشین\\u200cآلات',\n",
              " 'کالا',\n",
              " 'مبتلا',\n",
              " 'لاتین',\n",
              " 'اسلاوی',\n",
              " 'مالاگا',\n",
              " 'لاذقیه',\n",
              " 'langarاللاذقیة',\n",
              " 'عربیاللاذقیة',\n",
              " 'لاتینیLaodicea',\n",
              " 'للمیلاد',\n",
              " 'والاسلامیة',\n",
              " 'ایلام',\n",
              " 'گیلان',\n",
              " 'اردلانی',\n",
              " 'اصطلاح',\n",
              " 'اطلاق',\n",
              " 'مثلا',\n",
              " 'الاکرادref',\n",
              " 'فلات',\n",
              " 'الارض',\n",
              " 'الانبیا',\n",
              " 'الانبیاء',\n",
              " 'الصلاه',\n",
              " 'السلام',\n",
              " 'الاکراد',\n",
              " 'ولادمیر',\n",
              " 'ایلات',\n",
              " 'ملاطیه',\n",
              " 'احتمالا',\n",
              " 'ملا',\n",
              " 'لاتینی',\n",
              " 'تلاش',\n",
              " 'تازه\\u200cاستقلال\\u200cیافته',\n",
              " 'اختلاف',\n",
              " 'غلامرضا',\n",
              " 'تحولات',\n",
              " 'باجلانی\\u200cها',\n",
              " 'ولادیمیر',\n",
              " 'اطلاعات\\u200cنامه',\n",
              " 'طولانی',\n",
              " 'ایلامایلام',\n",
              " 'ملاحظه',\n",
              " 'دالاهو',\n",
              " 'مجلات',\n",
              " 'اختلاط',\n",
              " 'اخلاف',\n",
              " 'حاصلخیزهلال',\n",
              " 'اسلامناشر',\n",
              " 'ایلامسال',\n",
              " 'هه\\u200cلاله',\n",
              " 'مندالان',\n",
              " 'pxهلاله',\n",
              " 'هلاله',\n",
              " 'روژهه\\u200cلات',\n",
              " 'دلاور',\n",
              " 'سالار',\n",
              " 'بالا',\n",
              " 'لایق',\n",
              " 'کهلان',\n",
              " 'جلال',\n",
              " 'پلاک',\n",
              " 'سونگورلاره',\n",
              " 'زلاندنو',\n",
              " 'لارنسرودخانهٔ',\n",
              " 'لارنس',\n",
              " 'خلال',\n",
              " 'زلاند',\n",
              " 'ملاقات',\n",
              " 'سولاندر',\n",
              " 'بلافاصله',\n",
              " 'عملا',\n",
              " 'کیالاکه\\u200cکوا',\n",
              " 'دلایل',\n",
              " 'آلاسکا',\n",
              " 'سلاح',\n",
              " 'کلاسیککلاسیک',\n",
              " 'اصلاح',\n",
              " 'شدهاصلاح',\n",
              " 'بعلاوه',\n",
              " 'ایرلاینز',\n",
              " 'بلاویا',\n",
              " 'فلای\\u200cدبی',\n",
              " 'ایرلاینزSCAT',\n",
              " 'لارناکا',\n",
              " 'آمریکادلار',\n",
              " 'آمریکاایالات',\n",
              " 'بلامانع',\n",
              " 'ولایت',\n",
              " 'سال\\u200cولایت\\u200cشدن',\n",
              " 'علاقه\\u200cداری\\u200cها',\n",
              " 'مواصلاتی',\n",
              " 'مشکلات',\n",
              " 'ولایتی',\n",
              " 'بالامرغاب',\n",
              " 'حاصلات',\n",
              " 'الایام',\n",
              " 'فلادلفیا',\n",
              " 'ولایاتی',\n",
              " 'اصلا',\n",
              " 'جنگلات',\n",
              " 'الاخبار',\n",
              " 'حالا',\n",
              " 'بالاخره',\n",
              " 'بغلان',\n",
              " 'غلات',\n",
              " 'مشهورولایت',\n",
              " 'تحصیلات',\n",
              " 'فعلا',\n",
              " 'افغانستانبغلان',\n",
              " 'بلخیمولانا',\n",
              " 'مولانا',\n",
              " 'غلام\\u200cحضرت',\n",
              " 'pxولایت',\n",
              " 'پلانگذاری',\n",
              " 'سلامت',\n",
              " 'عبدالاحمد',\n",
              " 'مرکزولایت',\n",
              " 'تغییرمسیرانقلاب',\n",
              " 'اصلاحات',\n",
              " 'سکولار',\n",
              " 'اسلامیrefref',\n",
              " 'اسلامیref',\n",
              " 'لاهیجی',\n",
              " 'refrefمیلانی',\n",
              " 'دیوانسالاری',\n",
              " 'اصلاح\\u200cاندیش',\n",
              " 'محلات',\n",
              " 'علامه',\n",
              " 'میلانی',\n",
              " 'پکمیلانی',\n",
              " 'اصلاح\\u200cطلبان',\n",
              " 'انقلابیون',\n",
              " 'کلاس',\n",
              " 'استدلال',\n",
              " 'برخلاف',\n",
              " 'طلاب',\n",
              " 'اعلامیه',\n",
              " 'اصلاح\\u200cنشده',\n",
              " 'اسلامگرای',\n",
              " 'خلاص',\n",
              " 'انحلال',\n",
              " 'دلال',\n",
              " 'اعلان',\n",
              " 'بی\\u200cملاحظه',\n",
              " 'اصلاحگری',\n",
              " 'ارباب\\u200cسالاری',\n",
              " 'گلایه',\n",
              " 'اطلاع\\u200cرسانی',\n",
              " 'کلانتری',\n",
              " 'پالایشگاه',\n",
              " 'ائتلاف',\n",
              " 'لاهوتی',\n",
              " 'اسلامیکمیته',\n",
              " 'اطلاع',\n",
              " 'انقلابدادگاه\\u200cهای',\n",
              " 'ایرانانقلاب',\n",
              " 'ابلاغ',\n",
              " 'تشکیلاتی',\n",
              " 'ممنوع\\u200cالانتشار',\n",
              " 'اختلال',\n",
              " 'ملازم',\n",
              " 'فی\\u200cالارض',\n",
              " 'comfairواژههاییکهباجمهوریاسلامیبهایرانآمدندg',\n",
              " 'لاله',\n",
              " 'آمریکااعلام',\n",
              " 'بالاfont',\n",
              " 'لاتینو',\n",
              " 'گواتمالا',\n",
              " 'دلاویر',\n",
              " 'آمریکااعلامیه',\n",
              " 'فیلادلفیا',\n",
              " 'اصلاحیه',\n",
              " 'متقابلا',\n",
              " 'ونزوئلا',\n",
              " 'لابی',\n",
              " 'بالایی',\n",
              " 'دالاس',\n",
              " 'کلان\\u200cشهر',\n",
              " 'لانگ',\n",
              " 'آمریکاآتلانتیک',\n",
              " 'دالاسفورت',\n",
              " 'لادردیل',\n",
              " 'آتلانتا',\n",
              " 'کلانشهر',\n",
              " 'بلاعوض',\n",
              " 'دلاری',\n",
              " 'مبادلات',\n",
              " 'اقلا',\n",
              " 'میلادیref',\n",
              " 'فولاد',\n",
              " 'پالایش',\n",
              " 'دلارsup',\n",
              " 'سکولارکشور',\n",
              " 'اعلامیهٔ',\n",
              " 'اصلاحیهٔ',\n",
              " 'وارهولاندی',\n",
              " 'بنگلادش',\n",
              " 'خلاقیت',\n",
              " 'گولاش',\n",
              " 'کلانتر',\n",
              " 'Statesایالات',\n",
              " 'لاابالی\\u200cگری',\n",
              " 'تخیلات',\n",
              " 'الافکار',\n",
              " 'اطلال',\n",
              " 'اصطلاحا',\n",
              " 'نیوکلاسیک',\n",
              " 'غلامحسن',\n",
              " 'کلاسیک',\n",
              " 'جلال\\u200cالدین',\n",
              " 'بلاغت',\n",
              " 'غلامحسین',\n",
              " 'بطلان',\n",
              " 'مثلااستخوان',\n",
              " 'گلابی',\n",
              " 'سه\\u200cلایه\\u200cازجنس',\n",
              " 'غیراخلاقی',\n",
              " 'عضلات',\n",
              " 'اخلاقی',\n",
              " 'تالار',\n",
              " 'اولاند',\n",
              " 'htmsmallاطلاعات',\n",
              " 'اعلامیۀ',\n",
              " 'لاسکو',\n",
              " 'آتلانتیک',\n",
              " 'نیکولا',\n",
              " 'ماشینماشین\\u200cآلات',\n",
              " 'پرلاشز',\n",
              " 'پالاس',\n",
              " 'لاویل',\n",
              " 'فنلاندی',\n",
              " 'فنلاند',\n",
              " 'دلارref',\n",
              " 'بالاspan',\n",
              " 'شیلات',\n",
              " 'ژولای',\n",
              " 'لاتویا',\n",
              " 'بلاروس',\n",
              " 'سالانه\\u200cاست',\n",
              " 'صلاحیت',\n",
              " 'اختلافات',\n",
              " 'عادلانه',\n",
              " 'ناعادلانه',\n",
              " 'اصطلاحی',\n",
              " 'کلام',\n",
              " 'موزیلا',\n",
              " 'خلاء',\n",
              " 'پلانک',\n",
              " 'آلاینده',\n",
              " 'pxپلان',\n",
              " 'میلادref',\n",
              " 'طلایی',\n",
              " 'لای',\n",
              " 'کلاه',\n",
              " 'ملات',\n",
              " 'عیلامی',\n",
              " 'کلاه\\u200cتیزخود',\n",
              " 'طلا',\n",
              " 'لاجورد',\n",
              " 'دالان',\n",
              " 'والا',\n",
              " 'املایی',\n",
              " 'آنلاین',\n",
              " 'بلاغی',\n",
              " 'لاژورد',\n",
              " 'دولاب',\n",
              " 'روزآنلاین',\n",
              " 'اسلامیکاتاریختاریخ',\n",
              " 'تعطیلات',\n",
              " 'املا',\n",
              " 'نیلاندر',\n",
              " 'ملک\\u200cشاهجلال\\u200cالدین',\n",
              " 'جلالیتقویم',\n",
              " 'refعلامه',\n",
              " 'همشهری\\u200cآنلایننشانی',\n",
              " 'اسلامینشانی',\n",
              " 'میشلاوباماوتبریکنوروزبهزبانفارسی',\n",
              " 'کارلا',\n",
              " 'سلام',\n",
              " 'غلام',\n",
              " 'سالارمردم',\n",
              " 'بلاغ',\n",
              " 'اطلاع\\u200cرسانیref',\n",
              " 'بلال',\n",
              " 'سالاری',\n",
              " 'ربیع\\u200cالاول',\n",
              " 'محمدخلافت',\n",
              " 'عاملانی',\n",
              " 'جمادی\\u200cالاول',\n",
              " 'تجملات',\n",
              " 'نهج\\u200cالبلاغه',\n",
              " 'اسلامEncyclopaedia',\n",
              " 'فلان',\n",
              " 'refمقالات',\n",
              " 'الامکان',\n",
              " 'حسنکتابمقالات',\n",
              " 'علاوهٔ',\n",
              " 'موزیلاام\\u200cپی\\u200cال',\n",
              " 'مشکلاتی',\n",
              " 'پلاگین',\n",
              " 'غلاف',\n",
              " 'هسته\\u200cایسلاح\\u200cهای',\n",
              " 'بالاست',\n",
              " 'قلاع',\n",
              " 'کهنسالان',\n",
              " 'سیلاخور',\n",
              " 'واژگونلاله\\u200cهای',\n",
              " 'غلهغلات',\n",
              " 'قزل\\u200cآلا',\n",
              " 'خاتم\\u200cالانبیا',\n",
              " 'نیکلای',\n",
              " 'آلات',\n",
              " 'اسلامگرایان',\n",
              " 'رومیجلال',\n",
              " 'بالابان',\n",
              " 'پالاینده',\n",
              " 'پالایه',\n",
              " 'طلال',\n",
              " 'شکلات',\n",
              " 'کلان',\n",
              " 'صلاح',\n",
              " 'ائتلافی',\n",
              " 'الاقصی',\n",
              " 'لاتینلاتین',\n",
              " 'سینالا',\n",
              " 'لا',\n",
              " 'ملایم',\n",
              " 'چهارلایی',\n",
              " 'سه\\u200cلایی',\n",
              " 'گلابی\\u200cشکل',\n",
              " 'پلاستیک',\n",
              " 'لایه',\n",
              " '٫۴چهارلا',\n",
              " '٫۳چهارلا',\n",
              " '٫۹چهارلا',\n",
              " '٫۵چهارلا',\n",
              " 'چهارلا',\n",
              " 'لابمل',\n",
              " 'لاکرن',\n",
              " '٫۶چهارلا',\n",
              " '٫۰چهارلا',\n",
              " '٫۷چهارلا',\n",
              " '٫۲چهارلا',\n",
              " 'نتلا',\n",
              " 'ارسلان',\n",
              " 'جولان',\n",
              " 'اسلاممسلمان\\u200cها',\n",
              " 'میلادیطرح',\n",
              " 'اسرائیلاحزاب',\n",
              " 'سکولاریسمسیستم',\n",
              " 'سکولاریسم',\n",
              " 'برایلان',\n",
              " 'موتورولا',\n",
              " 'سالانهٔ',\n",
              " 'محصولاتی',\n",
              " 'کلاهک',\n",
              " 'کلاهک\\u200cهای',\n",
              " 'اسلاو',\n",
              " 'اسلاوها',\n",
              " 'لاتینی\\u200cسازی',\n",
              " 'استیلا',\n",
              " 'لاییک',\n",
              " 'آلبانیاستقلال',\n",
              " 'مالاکاستر',\n",
              " 'ولاش',\n",
              " 'لابئات',\n",
              " 'تاؤلانت',\n",
              " 'باهیابلانکا',\n",
              " 'پلاتا',\n",
              " 'لاپلاتا',\n",
              " 'آدلاید',\n",
              " 'براتیسلاوا',\n",
              " 'ماچالا',\n",
              " 'لاشنجه',\n",
              " 'لایپزیگ',\n",
              " 'ولا',\n",
              " 'آنگولا',\n",
              " 'کامپالا',\n",
              " 'لاس',\n",
              " 'اکلاهماسیتی',\n",
              " 'میلان',\n",
              " 'گلاسگو',\n",
              " 'لاپاز',\n",
              " 'لاهور',\n",
              " 'پوکاپالا',\n",
              " 'کالائو',\n",
              " 'دارالسلام',\n",
              " 'لانژو',\n",
              " 'ولادی\\u200cوستوک',\n",
              " 'سریلانکا',\n",
              " 'چیلان',\n",
              " 'کالاما',\n",
              " 'سلاله',\n",
              " 'بارانکیلا',\n",
              " 'سانتاکلارا',\n",
              " 'دلاس',\n",
              " 'آخالکالاکی',\n",
              " 'تلاوی',\n",
              " 'مالابو',\n",
              " 'لائوس',\n",
              " 'مالاوی',\n",
              " 'کوالالامپور',\n",
              " 'کازابلانکا',\n",
              " 'اولانباتار',\n",
              " 'گوادالاخارا',\n",
              " 'لاگوس',\n",
              " 'پورت\\u200cویلا',\n",
              " 'لاهه',\n",
              " 'سولا',\n",
              " 'اختلالات',\n",
              " 'اختلالاتی',\n",
              " 'همسالان',\n",
              " 'اسلاواسلاوها',\n",
              " 'الاءعلاق\\u200cالنفیسة',\n",
              " 'افلاک',\n",
              " 'لانتان',\n",
              " 'لانتانیدها',\n",
              " 'لانتانید',\n",
              " 'ولایتهلمند',\n",
              " 'علاقه\\u200cداری',\n",
              " 'اتولا',\n",
              " 'ملاهادی',\n",
              " 'خانم\\u200cبالا',\n",
              " 'طلاق',\n",
              " 'گفتاوردحالا',\n",
              " 'بلا',\n",
              " 'لیلا',\n",
              " 'غلامحسین\\u200cمیرزا',\n",
              " 'جلال\\u200cالممالک',\n",
              " 'لاغراندام',\n",
              " 'تمایلات',\n",
              " 'خلقطلاب',\n",
              " 'لابه\\u200cلا',\n",
              " 'گیلانی',\n",
              " 'کلاغ',\n",
              " 'افلاکی',\n",
              " 'روزمولاناستولیانگارنهانگارعنوانروز',\n",
              " 'مولاناست',\n",
              " 'میرعلایی',\n",
              " 'اخلاق',\n",
              " 'کلامی',\n",
              " 'تاویلات',\n",
              " 'زرکوبصلاح',\n",
              " 'زدندبلبلان',\n",
              " 'الا',\n",
              " 'اولا',\n",
              " 'استعلام',\n",
              " 'ملاحسین',\n",
              " 'جواهرالاسرار',\n",
              " 'زواهرالانوار',\n",
              " 'فاتح\\u200cالابیات',\n",
              " 'علا',\n",
              " 'به\\u200cحلال',\n",
              " 'سپه\\u200cسالاری',\n",
              " 'لاهجان',\n",
              " 'دلایلهشتگانهضرورتتغییرپولملی',\n",
              " 'لاین',\n",
              " 'لاینپالیزه',\n",
              " 'جولانگاه',\n",
              " 'کلاسیسم',\n",
              " 'کلاسیسیسم',\n",
              " 'بالائی',\n",
              " 'هیمالایا',\n",
              " 'ماکس\\u200cپلانک',\n",
              " 'داگلاس',\n",
              " 'متلاشی',\n",
              " 'الارب',\n",
              " 'کلارک',\n",
              " 'refنولا',\n",
              " 'کالاref',\n",
              " 'کلادون',\n",
              " 'کولادون',\n",
              " 'پلاستیکی',\n",
              " 'پلاستکی',\n",
              " 'نورلامپهای',\n",
              " 'موادلازم',\n",
              " 'لایه\\u200cنشانی',\n",
              " 'ابزارآلات',\n",
              " 'لاشارلاشار',\n",
              " 'لاشار',\n",
              " 'کلا',\n",
              " 'الاولی',\n",
              " 'فلامینگو',\n",
              " 'وکلا',\n",
              " 'علاقات',\n",
              " 'الإسلام',\n",
              " 'تشکیلات',\n",
              " 'اسلاماسلامی',\n",
              " 'آلمانآلمانی\\u200cالاصل',\n",
              " 'ملاءعام',\n",
              " 'صالحاسلام',\n",
              " 'بالارتبه\\u200cترین',\n",
              " 'شلاق',\n",
              " 'لاوری',\n",
              " 'ملایر',\n",
              " 'ملامحمد',\n",
              " 'لائودیسه',\n",
              " 'لااودیسه',\n",
              " 'چلاند&چل',\n",
              " 'فتوکلاژ',\n",
              " 'لاهیجان',\n",
              " 'خلایق',\n",
              " 'الاشرف',\n",
              " 'کربلا',\n",
              " 'معلا',\n",
              " 'الاقالیماحسن',\n",
              " 'الاسفزاری',\n",
              " 'دارالخلافه\\u200cاش',\n",
              " 'الاقالیم',\n",
              " 'اسلامص',\n",
              " 'ایرانیاسلامی',\n",
              " 'گیلاس',\n",
              " 'اعتلاء',\n",
              " 'اعتلا',\n",
              " 'مداخلات',\n",
              " 'اسلامیمدرن',\n",
              " 'نشانههایاسلامیدرمعماریوشهرسازیمشهدرعایتنمیشود',\n",
              " 'بازنگریمعماریوشهرسازیکلانشهرمشهدلزومتبدیلبهپایتختفرهنگیجهاناسلام',\n",
              " 'اسلامتاریخ',\n",
              " 'سیلاب',\n",
              " 'بالاسر',\n",
              " 'قولsmallبرخلاف',\n",
              " 'اشکالات',\n",
              " 'نونهالان',\n",
              " 'پالاندوز',\n",
              " 'ییلاقات',\n",
              " 'مناطقگانهشهرمشهدتفکیکمحلات',\n",
              " 'محلاتناشر',\n",
              " 'جالیزمحصولات',\n",
              " 'دارالولایه',\n",
              " 'جوادالائمه',\n",
              " 'خاتم\\u200cالانبیابیمارستان',\n",
              " 'خاتم\\u200cالانبیاء',\n",
              " 'آنلاینتاریخ',\n",
              " 'اسلامیتاریخ',\n",
              " 'اجلاس',\n",
              " 'مشهدپایتختفرهنگیجهاناسلاممیشودفرصتیناببرایترویجفرهنگ',\n",
              " 'اسلامیبنیاد',\n",
              " 'معضلات',\n",
              " 'ثامن\\u200cالائمه',\n",
              " 'سوالات',\n",
              " 'کلاتref',\n",
              " 'ایستگاهسوارکلات',\n",
              " 'درصدیمشکلاتزیستمحیطیدرمشهد',\n",
              " 'دلایلگسترشحاشیهنشینیدرمشهدتدوینسندتوانمندسازیسکونتگاههای',\n",
              " 'افزایشدرصدیطلاقدرمشهد',\n",
              " 'درصدطلاقهادرمشهداستراههایافزایشکیفیترابطهجنسی',\n",
              " 'بعدازاعتیادطلاقخشونتسومینآسیبعمدهاجتماعیدرمشهد',\n",
              " 'مشهدولاهورخواهرخواندهشدند',\n",
              " 'کمپوستلا',\n",
              " 'غلام\\u200cحسین',\n",
              " 'باب\\u200cالابواب',\n",
              " 'آنلاینrefref',\n",
              " 'سولاک',\n",
              " 'اصلیاختلاف',\n",
              " 'خبرآنلاینتاریخ',\n",
              " 'سفلای',\n",
              " 'کشورعملا',\n",
              " 'آلایندهٔ',\n",
              " 'پلانکتونها',\n",
              " 'لاکادیو',\n",
              " 'لاکشادویپLaccadive',\n",
              " 'میلا',\n",
              " 'سلاجقه',\n",
              " 'ارسلانعضدالدوله',\n",
              " 'ملازگرد',\n",
              " 'ملکشاهجلال\\u200cالدوله',\n",
              " 'الابنیه',\n",
              " 'الادویه',\n",
              " 'گیوهکلاش',\n",
              " 'اردلاناردلانی',\n",
              " 'قشلاق',\n",
              " 'بلافصل',\n",
              " 'اسلام\\u200cآباد',\n",
              " 'اردلاناردلان',\n",
              " 'غلامشاه',\n",
              " 'اردلان\\u200cها',\n",
              " 'شلایی',\n",
              " 'گلابتون\\u200cدوزی',\n",
              " 'هلاج',\n",
              " 'حلاج',\n",
              " 'چوارلان',\n",
              " 'ملاکاوو',\n",
              " 'لان',\n",
              " 'دهگلان',\n",
              " 'کلاقاه',\n",
              " 'کلاو',\n",
              " 'زیورآلات',\n",
              " 'پلان',\n",
              " 'امجدالاشراف',\n",
              " 'شیخ\\u200cالاسلام',\n",
              " 'ملالطف',\n",
              " 'قشلاقپل',\n",
              " 'دارالاحسان',\n",
              " 'دارالامان',\n",
              " 'علاقه\\u200cمندی',\n",
              " 'آوالان',\n",
              " 'قدیم\\u200cالایام',\n",
              " 'چهارلان',\n",
              " 'قشلاقسد',\n",
              " 'دارالایاله',\n",
              " 'محلاتمرکزی',\n",
              " 'محلاتی',\n",
              " 'خبرآنلاین',\n",
              " 'محلاتتاریخ',\n",
              " 'صدرالاشراف',\n",
              " 'محلاتیو',\n",
              " 'الاعتقاد',\n",
              " 'الاسلامی',\n",
              " 'فلاس',\n",
              " 'الاهی',\n",
              " 'کلامکلام',\n",
              " 'عقلانی',\n",
              " 'refاخلاق',\n",
              " 'خلاق',\n",
              " 'سؤالاتی',\n",
              " 'سؤالات',\n",
              " 'جملاتی',\n",
              " 'بلایای',\n",
              " 'جلا',\n",
              " 'ولانس',\n",
              " 'جلالوند',\n",
              " 'کلاوه',\n",
              " 'بلایای\\u200cهای',\n",
              " 'سری\\u200cلانکا',\n",
              " 'لانکا',\n",
              " 'پالارپالارها',\n",
              " 'عیلام',\n",
              " 'هیولا',\n",
              " 'تعاملات',\n",
              " 'اصولا',\n",
              " 'اتصالات',\n",
              " 'گیلانگیلان',\n",
              " 'اردبیلاردبیل',\n",
              " 'فولادزره',\n",
              " 'بوسلامه',\n",
              " 'اسلاواسلاوی',\n",
              " 'امیدسالار',\n",
              " 'الإسلامیة',\n",
              " 'الثقلان',\n",
              " 'آق\\u200cقلا',\n",
              " 'آق\\u200cقلاآق\\u200cقلا',\n",
              " 'آلاشت',\n",
              " 'فلاورجانفلاورجان',\n",
              " 'اردلاردل',\n",
              " 'ثلاث',\n",
              " 'باباجانیثلاث',\n",
              " 'ملایرملایر',\n",
              " 'غرباسلام\\u200cآباد',\n",
              " 'اسلام\\u200cشهر',\n",
              " 'اسلامشهراسلامشهر',\n",
              " 'اسلامیه',\n",
              " 'لامردلامرد',\n",
              " 'اصلاندوز',\n",
              " 'امیرکلا',\n",
              " 'لارستانلارستان',\n",
              " 'آریاشهربالاده',\n",
              " 'دهگلاندهگلان',\n",
              " 'چالانچولان',\n",
              " 'رستمکلا',\n",
              " 'سرخنکلاته',\n",
              " 'علامرودشت',\n",
              " 'فلاورجان',\n",
              " 'فولادشهر',\n",
              " 'کلاته',\n",
              " 'کلاچای',\n",
              " 'کلارآباد',\n",
              " 'کلاردشت',\n",
              " 'کلاله',\n",
              " 'کیاکلا',\n",
              " 'گیلانغرب',\n",
              " 'لار',\n",
              " 'لالجین',\n",
              " 'لال',\n",
              " 'لامرد',\n",
              " 'لاهرود',\n",
              " 'لیلان',\n",
              " 'مرزیکلا',\n",
              " 'معلم\\u200cکلایه',\n",
              " 'ملاثانی',\n",
              " 'ملارد',\n",
              " 'میلاجرد',\n",
              " 'اتومبیلایران',\n",
              " 'جلایر',\n",
              " 'بلاذری',\n",
              " 'ییلاق',\n",
              " 'قرلان',\n",
              " 'قشلاققشلاق',\n",
              " 'فاضلاب',\n",
              " 'حلال',\n",
              " 'الامر',\n",
              " 'میرعلام',\n",
              " 'ملامراد',\n",
              " 'لایحضره',\n",
              " 'الانوارالنعمانیه',\n",
              " 'دررالاشعار',\n",
              " 'کربلایی',\n",
              " 'مصلا',\n",
              " 'کلاهخود',\n",
              " 'سوته\\u200cدلان',\n",
              " 'مولویمولانا',\n",
              " 'مولاناشکل',\n",
              " 'الاصل',\n",
              " 'دلاویل',\n",
              " 'فیلارمونیک',\n",
              " 'ساپودیلا',\n",
              " 'امیرارسلان',\n",
              " 'اسطرلاب',\n",
              " 'زیلانت',\n",
              " 'لاخ',\n",
              " 'دراکولا',\n",
              " 'کوپولا',\n",
              " 'دراکولادراکولا',\n",
              " 'لایکن\\u200cها',\n",
              " 'بالابردن',\n",
              " 'نیلا',\n",
              " 'بالابلندتر',\n",
              " 'بلندبالایی',\n",
              " 'پائولا',\n",
              " 'وبلاگ',\n",
              " 'کتابلاگ',\n",
              " 'ملامت',\n",
              " 'متلاطم',\n",
              " 'آلاشکرت',\n",
              " 'ارمنستانآلاشکرت',\n",
              " 'املائی',\n",
              " 'لازاریانژانت',\n",
              " 'لازاریان',\n",
              " 'تبادلات',\n",
              " 'نام\\u200cرسمیکیلان',\n",
              " 'php۵titleکیلان',\n",
              " 'کالالیت',\n",
              " 'کالاآلیت',\n",
              " 'کالالیت\\u200cها',\n",
              " 'توپیلاک',\n",
              " 'کلالیت',\n",
              " 'پردلانه',\n",
              " 'کاملابوجعفر',\n",
              " 'اساس\\u200cالاقتباس',\n",
              " 'کلاممتکلم',\n",
              " 'کلاما',\n",
              " 'کلامکلامی',\n",
              " 'اسلامی\\u200cست',\n",
              " 'هلاکوهلاکوی',\n",
              " 'هلاکو',\n",
              " 'اعلاء',\n",
              " 'الاعراق',\n",
              " 'هلاکوخان',\n",
              " 'البلاغه',\n",
              " 'البلاغة',\n",
              " 'الالقاب',\n",
              " 'رسالاتی',\n",
              " 'الاقتباس',\n",
              " 'ذیلا',\n",
              " 'الاختیار',\n",
              " 'الاعتقادات',\n",
              " 'کلاممتکمین',\n",
              " 'تجریک\\u200cالکلام',\n",
              " 'بروکلاین',\n",
              " 'بولاک',\n",
              " 'کلایو',\n",
              " 'اصلانی',\n",
              " 'آن\\u200cلایم',\n",
              " 'ستلایت',\n",
              " 'گلاب',\n",
              " 'ماتلاک',\n",
              " 'پورتلاندیا',\n",
              " 'پدرسالار',\n",
              " 'دلایلکنارهگیریکتایونریاحیازبازیگریسینماآمدهتاقداستراازمذهبوعصمتراازانبیابگیردعنوان',\n",
              " 'ناجاپیگیریبدحجابیبازیگراندرخارجکشورازطریقاینترپلایجادقرارگاهویژهامنیتانتخابات',\n",
              " 'لاک\\u200cپشت',\n",
              " 'کارتالا',\n",
              " 'الببلاوی',\n",
              " 'الازهر',\n",
              " 'اسلامی\\u200cسازی',\n",
              " 'الاحمر',\n",
              " 'مردسالاریمردسالار',\n",
              " 'اصلیانقلاب',\n",
              " 'یونانیونانی\\u200cالاصل',\n",
              " 'ارلاخ',\n",
              " 'ازمیلاد',\n",
              " 'کلاستی',\n",
              " 'علاءالدین',\n",
              " 'علا\\u200cالدین',\n",
              " 'علاالدین',\n",
              " 'لاریجان',\n",
              " 'comجمهوریاسلامیایرانمازندرانآملcontents',\n",
              " 'دیولافوا',\n",
              " 'مداخلاتی',\n",
              " 'کالایی',\n",
              " 'comمردمانگیلانومازندراننژادکاس\\u200cپی',\n",
              " 'محتملا',\n",
              " 'نگاهیگذرابهسنتورفچالدرمازندرانزنسالارییکروزهدر',\n",
              " 'لاره',\n",
              " 'لاریجانی',\n",
              " 'غلامعلی',\n",
              " 'داعی\\u200cالاسلام',\n",
              " 'غلامی',\n",
              " 'تسهیلات',\n",
              " 'الامام',\n",
              " 'فخرالاسلام',\n",
              " 'علاو',\n",
              " 'چلابی',\n",
              " 'الاصغر',\n",
              " 'دلارستاقی',\n",
              " 'ملاط',\n",
              " 'اضلاع',\n",
              " 'آملامامزاده',\n",
              " 'فیروزکلا',\n",
              " 'علاءالدوله',\n",
              " 'تلار',\n",
              " 'هندوکلاتکیه',\n",
              " 'هندوکلا',\n",
              " 'قاضی\\u200cکلا',\n",
              " 'دلارستاق',\n",
              " 'چلاو',\n",
              " 'آلاچیق',\n",
              " 'آلامل',\n",
              " 'پلاژ',\n",
              " 'میلادسه',\n",
              " 'comمقالاتمدارسنظامیه',\n",
              " 'درصدصنعتمازندراندرآملاس',\n",
              " 'لاک',\n",
              " 'comمحصولاتمازندران',\n",
              " 'سلطانمسعودتعطیلاتشراچگونهمی\\u200cگذراند',\n",
              " 'افلاطون',\n",
              " 'پلاتون',\n",
              " 'افلاطونref',\n",
              " 'افلاطون\\u200cشناسان',\n",
              " 'گلاوکن',\n",
              " 'معقولات',\n",
              " 'افلاطون\\u200cگرایان',\n",
              " 'کولاب',\n",
              " 'شلایشر',\n",
              " 'کلارا',\n",
              " 'پاولا',\n",
              " 'هیتلرپاولا',\n",
              " 'جملات',\n",
              " 'ایالات\\u200cها',\n",
              " 'منفعلانه',\n",
              " 'علایق',\n",
              " 'pxبندانگشتیاعلام',\n",
              " 'قلادهٔ',\n",
              " 'بلاندی',\n",
              " 'آنگلا',\n",
              " 'هیتلرآنگلا',\n",
              " 'لابراتوار',\n",
              " 'اسلامیکرج',\n",
              " 'بلاواتسکی',\n",
              " 'ضداطلاعات',\n",
              " 'فالانژیسم',\n",
              " 'مسجدالاقصی',\n",
              " 'لاشه',\n",
              " 'لازمه',\n",
              " 'اسلامیمجموعه',\n",
              " 'مقالاتص',\n",
              " 'مشرق\\u200cالاذکار',\n",
              " 'استقلال\\u200cطلبانه',\n",
              " 'میلادیماه',\n",
              " 'گه\\u200cلاویژ',\n",
              " 'گه\\u200cلارێزان',\n",
              " 'گه\\u200cلاوێژ',\n",
              " 'اردیبهشتگولان',\n",
              " 'لاویژ',\n",
              " 'لائیک',\n",
              " 'کلاسوس',\n",
              " 'میلادپ',\n",
              " 'اسلامبول',\n",
              " 'سکولاریزماصل',\n",
              " 'مردمسالاری',\n",
              " 'لازها',\n",
              " 'آلپ\\u200cارسلان',\n",
              " 'سکولاریسمسکولار',\n",
              " 'اسلامیسنی',\n",
              " 'آلانیا',\n",
              " 'گالاتاسرای',\n",
              " 'مالاتیا',\n",
              " 'موغلا',\n",
              " 'انگولا',\n",
              " 'گوآتمالا',\n",
              " 'فلاند',\n",
              " 'توکلائو',\n",
              " 'ملانزی',\n",
              " 'پالائو',\n",
              " 'جیلان',\n",
              " 'پولاد',\n",
              " 'بوزارسلان',\n",
              " 'هاسپولادلی',\n",
              " 'باجلانیزبان',\n",
              " 'باجلانی',\n",
              " 'مستقلا',\n",
              " 'خلاصه\\u200cشده',\n",
              " 'لازلو',\n",
              " 'لاپتون',\n",
              " 'استدلالی',\n",
              " 'خلاقه',\n",
              " 'بلامنازع',\n",
              " 'علامت\\u200cگذاری',\n",
              " 'املاک',\n",
              " 'نشودباجلان',\n",
              " 'nameباجلانی',\n",
              " 'طلاسازی',\n",
              " 'الازیغالازیغ',\n",
              " 'استانبولاستانبول',\n",
              " 'ایراناستیلای',\n",
              " 'لاشکار',\n",
              " 'لانه',\n",
              " 'آزاداسلامی',\n",
              " 'سلامتی',\n",
              " 'ازاسلام',\n",
              " 'جلال\\u200cآباد',\n",
              " 'لاله\\u200cزار',\n",
              " 'مولا',\n",
              " 'شهلا',\n",
              " 'متوسلانی',\n",
              " 'کاپولا',\n",
              " 'الازیغ',\n",
              " 'stateایالات',\n",
              " 'قیزلاردفه',\n",
              " 'غلاتی',\n",
              " 'آلاداغ',\n",
              " 'گوکلان',\n",
              " 'فالاریس',\n",
              " 'قلا',\n",
              " 'بولارسنگایلده',\n",
              " 'بولارسنگ',\n",
              " 'بولارسنگقازانارسنگ',\n",
              " 'جرگلان',\n",
              " 'ولایتکابل',\n",
              " 'کلاودیوس',\n",
              " 'قولاسکندر',\n",
              " 'کادفیزسکوجولا',\n",
              " 'مایملاک',\n",
              " 'کالاله',\n",
              " 'بولان',\n",
              " 'بلاوقفه',\n",
              " 'ویلا',\n",
              " 'بالاحصار',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rz7JnPLJneY",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
        "تابع\n",
        "add_document_to_indexes\n",
        "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
        "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3V8mUFHJneZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_document_to_indexes(docs_path, doc_num):\n",
        "    docs_path = docs_path + \"/\" + str(doc_num) + \".xml\"\n",
        "    doc = parse(docs_path)\n",
        "    page = doc.getElementsByTagName('page')[0]\n",
        "    doc_id = read_page(page)[0]\n",
        "    if doc_id in validation.keys():\n",
        "        if validation[doc_id]:\n",
        "            raise Exception(\"This document is already in corpus!\")\n",
        "        else:\n",
        "            validation[doc_id] = True\n",
        "            global number_of_docs\n",
        "            number_of_docs += 1\n",
        "            return\n",
        "    auxiliary_positional_index, auxiliary_bigram_index = construct_positional_indexes(docs_path)\n",
        "    # merge\n",
        "    for index in auxiliary_positional_index.keys():\n",
        "        if index in positional_index.keys():\n",
        "            positional_index[index][doc_id] = auxiliary_positional_index[index][doc_id]\n",
        "        else:\n",
        "            positional_index[index] = {doc_id: auxiliary_positional_index[index][doc_id]}\n",
        "    for bigram in auxiliary_bigram_index.keys():\n",
        "        if bigram in bigram_index.keys():\n",
        "            for word in auxiliary_bigram_index[bigram]:\n",
        "                if word not in auxiliary_bigram_index[bigram]:\n",
        "                    bigram_index[bigram].append(word)\n",
        "        else:\n",
        "            bigram_index[bigram] = auxiliary_bigram_index[bigram]\n",
        "\n",
        "\n",
        "add_document_to_indexes('data/wiki', 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI62QI7FJnec",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش برای حذف کردن یک سند از نمایه است.\n",
        "تابع\n",
        "delete_document_from_indexes\n",
        "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
        "در صورتی که پس از حذف یک سند، \n",
        "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
        "به طور کامل حذف شود.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ_8rPAxJneg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_document_from_indexes(docs_path, doc_num):\n",
        "    docs_path = docs_path + \"/\" + str(doc_num) + \".xml\"\n",
        "    doc = parse(docs_path)\n",
        "    page = doc.getElementsByTagName('page')[0]\n",
        "    doc_id = read_page(page)[0]\n",
        "    validation[doc_id] = False\n",
        "    global number_of_docs\n",
        "    number_of_docs -= 1\n",
        "\n",
        "\n",
        "delete_document_from_indexes('data/wiki', 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL-19qvrJnek",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
        "و نیازی به ذخیره‌سازی نمایه \n",
        "Bigram نیست \n",
        ".\n",
        "تابع \n",
        "save_index\n",
        "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
        "destination\n",
        "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UestOXpOJnel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_index(destination):\n",
        "    with open(destination + \"/positional_index.txt\", 'w', encoding='utf8') as file:\n",
        "        dump(positional_index, file, ensure_ascii=False)\n",
        "    with open(destination + \"/bigram_index.txt\", 'w', encoding='utf8') as file:\n",
        "        dump(bigram_index, file, ensure_ascii=False)\n",
        "    with open(destination + \"/validation.txt\", 'w', encoding='utf8') as file:\n",
        "        dump(validation, file, ensure_ascii=False)\n",
        "\n",
        "\n",
        "save_index('storage/index_backup')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPL51GrmJnep",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
        "load_index\n",
        "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
        "source\n",
        "نمایه را از این فایل بارگذاری می‌کند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahonOMFMJneq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_index(source):\n",
        "    with open(source + \"/positional_index.txt\", 'r', encoding='utf8') as file:\n",
        "        loaded_positional_index = load(file)\n",
        "    with open(source + \"/bigram_index.txt\", 'r', encoding='utf8') as file:\n",
        "        loaded_bigram_index = load(file)\n",
        "    with open(source + \"/validation.txt\", 'r', encoding='utf8') as file:\n",
        "        loaded_validation = load(file)\n",
        "    if loaded_positional_index is None or loaded_validation is None or loaded_bigram_index is None:\n",
        "        raise Exception(\"Failed to load index\")\n",
        "    global positional_index\n",
        "    positional_index = loaded_positional_index\n",
        "    global bigram_index\n",
        "    bigram_index = loaded_bigram_index\n",
        "    global validation\n",
        "    validation = loaded_validation\n",
        "    global number_of_docs\n",
        "    number_of_docs = len(validation.keys())\n",
        "\n",
        "\n",
        "load_index('storage/index_backup')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5JeSxd2Jnet",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
        "</font>\n",
        "<hr>\n",
        "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
        "(*امتیازی*)\n",
        "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
        "<br>\n",
        "<br>\n",
        "<font color=red size=6>\n",
        "(*امتیازی*)\n",
        "اصلاح پرسمان\n",
        "</font>\n",
        "<br>\n",
        "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
        "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
        "bigram\n",
        "(با استفاده از معیار فاصله جاکارد) \n",
        "انتخاب شده و سپس\n",
        "بهترین آنها با معیار \n",
        "edit distance\n",
        "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
        "<br>\n",
        "<br>\n",
        "<font color=red size=6>\n",
        "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
        "</font>\n",
        "<br>\n",
        " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
        "<br>\n",
        "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
        "(ltn-lnn\n",
        "و\n",
        "ltc-lnc)\n",
        "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
        "<br>\n",
        "<br>\n",
        "<font color=red size=6>\n",
        "جستجوی دقیق \n",
        "(phrasal search)\n",
        "</font>\n",
        "<br>\n",
        "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
        "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
        "<br>\n",
        "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
        "عنوان نمونه برای پرسمان\n",
        "<br>\n",
        "\"q5 q4\" q3 \"q2 q1\"\n",
        "<br>\n",
        "سند\n",
        "<br>\n",
        "q3 q2 q1 q5 q4\n",
        "<br>\n",
        "مرتبط محسوب می‌شود. \n",
        "<br>\n",
        "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
        "</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UnpNX56Jneu",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "(*امتیازی*)\n",
        "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
        "correct_query\n",
        "پرسمان کاربر  \n",
        "(query)\n",
        "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5kybrGDJnev",
        "colab_type": "code",
        "outputId": "c8b886dc-46ec-483f-98cd-71fbe4a0bcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def jaccard(token_bigrams):\n",
        "    candidates = []\n",
        "    best_candidates = []\n",
        "    candidates_jaccard = []\n",
        "    for bigram in token_bigrams:\n",
        "        if bigram in bigram_index.keys():\n",
        "            for word in bigram_index[bigram]:\n",
        "                word_bigrams = set(get_bigrams(word))\n",
        "                intersect = len(token_bigrams & word_bigrams)\n",
        "                union = len(token_bigrams | word_bigrams)\n",
        "                candidates.append(word)\n",
        "                candidates_jaccard.append(intersect / union)\n",
        "    threshold = max(candidates_jaccard) - 0.1\n",
        "    for i in range(len(candidates)):\n",
        "        if candidates_jaccard[i] >= threshold:\n",
        "            best_candidates.append(candidates[i])\n",
        "    return set(best_candidates)\n",
        "\n",
        "\n",
        "def edit_distance(token, word, m, n):\n",
        "    insert, remove, change = 2, 2, 1\n",
        "    dp = [[0 for _ in range(n + 1)] for __ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0:\n",
        "                dp[i][j] = j\n",
        "            elif j == 0:\n",
        "                dp[i][j] = i\n",
        "            elif token[i - 1] == word[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = min(dp[i][j - 1] + insert, dp[i - 1][j] + remove, dp[i - 1][j - 1] + change)\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "def get_term_frequency_in_collection(word):\n",
        "    posting_list = get_posting_list(word)\n",
        "    frequency = 0\n",
        "    for doc in posting_list.values():\n",
        "        if \"title\" in doc.keys():\n",
        "            frequency += len(doc['title'])\n",
        "        if \"text\" in doc.keys():\n",
        "            frequency += len(doc['text'])\n",
        "    return frequency\n",
        "\n",
        "\n",
        "def correct_query(query):\n",
        "    corrected_query = \"\"\n",
        "    tokens = preprocess(query)\n",
        "    for token in tokens:\n",
        "        token_bigrams = set(get_bigrams(token))\n",
        "        if token not in positional_index.keys():\n",
        "            candidates = list(jaccard(token_bigrams))\n",
        "            edit_distance_of_candidates = []\n",
        "            best = []\n",
        "            weights = []\n",
        "            for candidate in candidates:\n",
        "                edit_distance_of_candidates.append(edit_distance(token, candidate, len(token), len(candidate)))\n",
        "            min_edit_distance = min(edit_distance_of_candidates)\n",
        "            for i in range(len(candidates)):\n",
        "                if edit_distance_of_candidates[i] == min_edit_distance:\n",
        "                    best.append(candidates[i])\n",
        "                    weights.append(get_term_frequency_in_collection(candidates[i]))\n",
        "            weights = [weights[i] / sum(weights) for i in range(len(weights))]\n",
        "            corrected_query += choice(best, p=weights) + \" \"\n",
        "        else:\n",
        "            corrected_query += token + \" \"\n",
        "    return corrected_query\n",
        "\n",
        "\n",
        "correct_query(\"شلام حالا برسهان درسک شد\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'سلام حالا برسلان درک شد '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9HMqDMZJney",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
        "search\n",
        "به عنوان اولین پارامتر پرسمان \n",
        "(query)\n",
        "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
        "در صورتی که درون پرسمان بخشی داخل\n",
        "\"\"\n",
        "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
        "پارامتر دوم ورودی روش محاسبه امتیاز \n",
        "(method)\n",
        "است که می‌تواند یکی از دو مقدار\n",
        "ltn-lnn\n",
        "و\n",
        "ltc-lnc\n",
        "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
        "پارامتر سوم \n",
        "(weight)\n",
        "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsgz32PDJnez",
        "colab_type": "code",
        "outputId": "87ccd996-289e-4166-c921-ec6092774a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "def does_contains_phrase(docs, terms, doc_id):\n",
        "    for position in docs[terms[0]][doc_id]:\n",
        "        i = 1\n",
        "        exists = True\n",
        "        while i < len(terms) and exists:\n",
        "            if position + i not in docs[terms[i]][doc_id]:\n",
        "                exists = False\n",
        "            i += 1\n",
        "        if exists:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_docs_contains_phrase(phrase):\n",
        "    terms = preprocess(phrase)\n",
        "    text_docs = {}\n",
        "    title_docs = {}\n",
        "    for term in terms:\n",
        "        docs = get_posting_list(term)\n",
        "        title_docs[term] = {}\n",
        "        text_docs[term] = {}\n",
        "        for doc_id in docs.keys():\n",
        "            if 'text' in docs[doc_id].keys():\n",
        "                text_docs[term][doc_id] = docs[doc_id]['text']\n",
        "            if 'title' in docs[doc_id].keys():\n",
        "                title_docs[term][doc_id] = docs[doc_id]['title']\n",
        "    text_id_intersect = set(text_docs[list(text_docs.keys())[0]].keys())\n",
        "    title_id_intersect = set(title_docs[list(title_docs.keys())[0]].keys())\n",
        "    for key in text_docs.keys():\n",
        "        text_id_intersect = text_id_intersect & set(text_docs[key].keys())\n",
        "    for key in title_docs.keys():\n",
        "        title_id_intersect = title_id_intersect & set(title_docs[key].keys())\n",
        "    phrase_in_text = set()\n",
        "    phrase_in_title = set()\n",
        "    for doc_id in text_id_intersect:\n",
        "        if does_contains_phrase(text_docs, terms, doc_id):\n",
        "            phrase_in_text.add(doc_id)\n",
        "    for doc_id in title_id_intersect:\n",
        "        if does_contains_phrase(title_docs, terms, doc_id):\n",
        "            phrase_in_title.add(doc_id)\n",
        "    return phrase_in_text, phrase_in_title\n",
        "\n",
        "\n",
        "def phrasal_search(phrases):\n",
        "    phrases_text_docs = {}\n",
        "    phrases_title_docs = {}\n",
        "    for phrase in phrases:\n",
        "        phrases_text_docs[phrase], phrases_title_docs[phrase] = get_docs_contains_phrase(phrase)\n",
        "    relevant_text_docs = phrases_text_docs[list(phrases_text_docs.keys())[0]]\n",
        "    relevant_title_docs = phrases_title_docs[list(phrases_title_docs.keys())[0]]\n",
        "    for key in phrases_text_docs.keys():\n",
        "        relevant_text_docs = relevant_text_docs & phrases_text_docs[key]\n",
        "    for key in phrases_title_docs.keys():\n",
        "        relevant_title_docs = relevant_title_docs & phrases_title_docs[key]\n",
        "    relevant_docs = relevant_title_docs.union(relevant_text_docs)\n",
        "    return relevant_text_docs, relevant_title_docs, relevant_docs\n",
        "\n",
        "\n",
        "def doc_tf(term, doc_id, text):\n",
        "    occurrence = get_posting_list(term)\n",
        "    if doc_id not in occurrence.keys():\n",
        "        return 0\n",
        "    if text:\n",
        "        tf = len(occurrence[doc_id]['text']) if 'text' in occurrence[doc_id].keys() else 0\n",
        "    else:\n",
        "        tf = len(occurrence[doc_id]['title']) if 'title' in occurrence[doc_id].keys() else 0\n",
        "    return 1 + log10(tf) if tf > 0 else 0\n",
        "\n",
        "\n",
        "def doc_idf(term, text, mode):\n",
        "    posting_list = get_posting_list(term)\n",
        "    if text:\n",
        "        key = 'text'\n",
        "    else:\n",
        "        key = 'title'\n",
        "    if mode == 'n':\n",
        "        return 1\n",
        "    elif mode == 't':\n",
        "        df = 0\n",
        "        for doc_id in posting_list:\n",
        "            if key in posting_list[doc_id].keys():\n",
        "                df += 1\n",
        "        return log10(number_of_docs / df) if df > 0 else 0\n",
        "\n",
        "\n",
        "def query_tf(term, query_terms):\n",
        "    occurrence = 0\n",
        "    for query_term in query_terms:\n",
        "        if query_term == term:\n",
        "            occurrence += 1\n",
        "    return 1 + log10(occurrence) if occurrence > 0 else 0\n",
        "\n",
        "\n",
        "def doc_score(doc_id, query_terms, method_for_doc, method_for_query, text, query_vector, idf):\n",
        "    doc_vector = []\n",
        "    for i in range(len(query_terms)):\n",
        "        doc_vector.append(doc_tf(query_terms[i], doc_id, text) * idf[i])\n",
        "    score = dot(doc_vector, query_vector)\n",
        "    doc_length = norm(doc_vector)\n",
        "    query_length = norm(query_vector)\n",
        "    if method_for_doc[2] == 'c':\n",
        "        score = score / doc_length if doc_length > 0 else 0\n",
        "    if method_for_query[2] == 'c':\n",
        "        score = score / query_length if query_length > 0 else 0\n",
        "    return score\n",
        "\n",
        "\n",
        "def search(query, method=\"ltn-lnn\", weight=2, number_of_results=20):\n",
        "    relevant_docs = []\n",
        "    method_for_doc, method_for_query = method.split('-')\n",
        "    heap = []\n",
        "    heapify(heap)\n",
        "\n",
        "    # query = correct_query(query)\n",
        "    phrases = findall('\"([^\"]*)\"', query)\n",
        "    query_terms = preprocess(query)\n",
        "\n",
        "    # find all docs which contain at least one of terms\n",
        "    all_docs = set(get_posting_list(query_terms[0]).keys())\n",
        "    for query_term in query_terms:\n",
        "        all_docs = all_docs.union(set(get_posting_list(query_term).keys()))\n",
        "\n",
        "    # check if there's phrases in queries. if there's, choose docs that contain them.\n",
        "    if len(phrases) > 0:\n",
        "        doc_contains_phrases = phrasal_search(phrases)[2]\n",
        "    else:\n",
        "        doc_contains_phrases = all_docs\n",
        "\n",
        "    # remove invalid docs\n",
        "    for doc_id in doc_contains_phrases:\n",
        "        if not validation[doc_id]:\n",
        "            doc_contains_phrases.remove(doc_id)\n",
        "\n",
        "    # calculate idf\n",
        "    text_idf = []\n",
        "    title_idf = []\n",
        "    for i in range(len(query_terms)):\n",
        "        text_idf.append(doc_idf(query_terms[i], True, method_for_doc[1]))\n",
        "        title_idf.append(doc_idf(query_terms[i], False, method_for_doc[1]))\n",
        "\n",
        "    # calculate query vector for text_idf and title_idf\n",
        "    query_vector_for_text = []\n",
        "    query_vector_for_title = []\n",
        "    for i in range(len(query_terms)):\n",
        "        query_vector_for_text.append(query_tf(query_terms[i], query_terms) * text_idf[i])\n",
        "        query_vector_for_title.append(query_tf(query_terms[i], query_terms) * title_idf[i])\n",
        "\n",
        "    # search\n",
        "    for doc_id in doc_contains_phrases:\n",
        "        score = doc_score(doc_id, query_terms, method_for_doc, method_for_query, True, query_vector_for_text, text_idf)\\\n",
        "                + weight * doc_score(doc_id, query_terms, method_for_doc, method_for_query, False, query_vector_for_title, title_idf)\n",
        "        if len(heap) < number_of_results:\n",
        "            heappush(heap, (score, doc_id))\n",
        "        elif heap[0][0] < score:\n",
        "            heappop(heap)\n",
        "            heappush(heap, (score, doc_id))\n",
        "    for i in range(len(heap)):\n",
        "        relevant_docs.insert(0, heappop(heap)[1])\n",
        "    return relevant_docs\n",
        "\n",
        "\n",
        "search('نظرخواهی انجام شده توسط دانشگاه \"شهر نیویورک\"', \"ltc-lnc\", 3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6770',\n",
              " '6824',\n",
              " '3103',\n",
              " '3404',\n",
              " '3797',\n",
              " '6810',\n",
              " '7049',\n",
              " '6889',\n",
              " '6074',\n",
              " '5904']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXhfuP0Jne5",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
        "detailed_search\n",
        "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
        "(title_query)\n",
        "و پرسمان بر روی متن\n",
        "(text_query)\n",
        "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
        "در صورتی که درون پرسمان بخشی داخل\n",
        "\"\"\n",
        "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
        "پارامتر دوم ورودی روش محاسبه امتیاز \n",
        "(method)\n",
        "است که می‌تواند یکی از دو مقدار\n",
        "ltn-lnn\n",
        "و\n",
        "ltc-lnc\n",
        "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmOjg1gYJne6",
        "colab_type": "code",
        "outputId": "15707649-4f62-453a-9b22-cc1115c66cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def detailed_search(title_query, text_query, method=\"ltn-lnn\", number_of_results=20):\n",
        "    text_relevant_docs_scores = {}\n",
        "    title_relevant_docs_scores = {}\n",
        "    relevant_docs = []\n",
        "\n",
        "    # query correction\n",
        "    # text_query = correct_query(text_query)\n",
        "    # title_query = correct_query(title_query)\n",
        "\n",
        "    text_query_phrases = findall('\"([^\"]*)\"', text_query)\n",
        "    title_query_phrases = findall('\"([^\"]*)\"', title_query)\n",
        "    method_for_doc, method_for_query = method.split('-')\n",
        "    heap = []\n",
        "    heapify(heap)\n",
        "\n",
        "    # preprocess\n",
        "    text_query_terms = preprocess(text_query)\n",
        "    title_query_terms = preprocess(title_query)\n",
        "\n",
        "    # find all docs which contain at least one of terms of text_query\n",
        "    text_all_docs = set(get_posting_list(text_query_terms[0]).keys())\n",
        "    for query_term in text_query_terms:\n",
        "        text_all_docs = text_all_docs | set(get_posting_list(query_term).keys())\n",
        "\n",
        "    # find all docs which contain at least one of terms of title_query\n",
        "    title_all_docs = set(get_posting_list(title_query_terms[0]).keys())\n",
        "    for query_term in title_query_terms:\n",
        "        title_all_docs = title_all_docs | set(get_posting_list(query_term).keys())\n",
        "\n",
        "    # check if there's phrases in queries. if there's, choose docs that contain them.\n",
        "    if len(text_query_phrases) > 0:\n",
        "        doc_contains_text_phrases = phrasal_search(text_query_phrases)[0]\n",
        "    else:\n",
        "        doc_contains_text_phrases = text_all_docs\n",
        "    if len(title_query_phrases) > 0:\n",
        "        doc_contains_title_phrases = phrasal_search(title_query_phrases)[0]\n",
        "    else:\n",
        "        doc_contains_title_phrases = title_all_docs\n",
        "\n",
        "    # remove invalid docs\n",
        "    for doc_id in doc_contains_text_phrases:\n",
        "        if not validation[doc_id]:\n",
        "            doc_contains_text_phrases.remove(doc_id)\n",
        "    for doc_id in doc_contains_title_phrases:\n",
        "        if not validation[doc_id]:\n",
        "            doc_contains_title_phrases.remove(doc_id)\n",
        "\n",
        "    # calculate idf\n",
        "    text_idf = []\n",
        "    title_idf = []\n",
        "    for i in range(len(text_query_terms)):\n",
        "        text_idf.append(doc_idf(text_query_terms[i], True, method_for_doc[1]))\n",
        "    for i in range(len(title_query_terms)):\n",
        "        title_idf.append(doc_idf(title_query_terms[i], False, method_for_doc[1]))\n",
        "\n",
        "    # calculate text_query vector and title_query vector\n",
        "    text_query_vector = []\n",
        "    for i in range(len(text_query_terms)):\n",
        "        text_query_vector.append(query_tf(text_query_terms[i], text_query_terms) * text_idf[i])\n",
        "    title_query_vector = []\n",
        "    for i in range(len(title_query_terms)):\n",
        "        title_query_vector.append(query_tf(title_query_terms[i], title_query_terms) * title_idf[i])\n",
        "\n",
        "    # search text_query\n",
        "    for doc_id in doc_contains_text_phrases:\n",
        "        score = doc_score(doc_id, text_query_terms, method_for_doc, method_for_query, True, text_query_vector, text_idf)\n",
        "        if score > 0:\n",
        "            text_relevant_docs_scores[doc_id] = score\n",
        "\n",
        "    # search title_query\n",
        "    for doc_id in doc_contains_title_phrases:\n",
        "        score = doc_score(doc_id, title_query_terms, method_for_doc, method_for_query, False, title_query_vector, title_idf)\n",
        "        if score > 0:\n",
        "            title_relevant_docs_scores[doc_id] = score\n",
        "\n",
        "    # merge docs that are relevant to text_query and relevant to title_query\n",
        "    common_docs = set(text_relevant_docs_scores.keys()) & set(title_relevant_docs_scores.keys())\n",
        "\n",
        "    # merge ranking results\n",
        "    for doc_id in common_docs:\n",
        "        score = text_relevant_docs_scores[doc_id] + title_relevant_docs_scores[doc_id]\n",
        "        if len(heap) < number_of_results:\n",
        "            heappush(heap, (score, doc_id))\n",
        "        elif heap[0][0] < score:\n",
        "            heappop(heap)\n",
        "            heappush(heap, (score, doc_id))\n",
        "    for i in range(len(heap)):\n",
        "        relevant_docs.insert(0, heappop(heap)[1])\n",
        "    return relevant_docs\n",
        "\n",
        "\n",
        "detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3854']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAO-xvEJne-",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
        "</font>\n",
        "<hr>\n",
        "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
        "<ol>\n",
        "<li>\n",
        "MAP\n",
        "</li>\n",
        "<li>\n",
        "F-Measure\n",
        "</li>\n",
        "<li>\n",
        "R-Precision\n",
        "</li>\n",
        "<li>\n",
        "NDCG\n",
        "</li>\n",
        "</ol>\n",
        "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
        "search \n",
        "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
        "detailed_search\n",
        "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
        "<br>\n",
        "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SPG2Mb3XVNny"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
        "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
        "all\n",
        "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnUndDEmgpkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_queries = {}\n",
        "all_relevants = {}\n",
        "all_results = {}\n",
        "\n",
        "\n",
        "def read_sample_queries():\n",
        "    for file in glob('data/queries/*.txt'):\n",
        "        query_id = file.replace('\\\\', '/').split('/')[2].replace('.txt', '')\n",
        "        with open(file, 'r', encoding='utf8') as query_file:\n",
        "            query = query_file.read()\n",
        "            all_queries[query_id] = query\n",
        "        with open('data/relevance/' + query_id + '.txt', 'r', encoding='utf8') as relevant_file:\n",
        "            all_relevants[query] = relevant_file.read().replace('\\n', '').replace(' ', '').split(',')\n",
        "\n",
        "\n",
        "def get_results_of_sample_queries():\n",
        "    for query in all_queries.values():\n",
        "        if query.__contains__('\\n'):\n",
        "            parts = query.split('\\n')\n",
        "            query_results = detailed_search(parts[0], parts[1])\n",
        "        else:\n",
        "            query_results = search(query)\n",
        "        all_results[query] = query_results\n",
        "\n",
        "\n",
        "read_sample_queries()\n",
        "get_results_of_sample_queries()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqaOk4ESJnfA",
        "colab_type": "code",
        "outputId": "5a9db8a3-1ae3-41fc-dcba-3a3e9a14fe31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def R_Precision(query_id='all'):\n",
        "    queries = list(all_queries.values()) if query_id == 'all' else [all_queries[query_id]]\n",
        "    relevants = all_relevants if query_id == 'all' else {all_queries[query_id]: all_relevants[all_queries[query_id]]}\n",
        "    results = all_results if query_id == 'all' else {all_queries[query_id]: all_results[all_queries[query_id]]}\n",
        "    evaluation_result = 0\n",
        "    for query in queries:\n",
        "        evaluation_result += len(set(results[query]) & set(relevants[query])) / len(relevants[query])\n",
        "    evaluation_result /= len(queries)\n",
        "    return evaluation_result\n",
        "\n",
        "\n",
        "print(R_Precision())\n",
        "print(R_Precision('1'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5309955489145993\n",
            "0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oMBkCclDgXf",
        "colab_type": "code",
        "outputId": "da6d426e-ffec-40e8-84c1-39c538562d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def F_measure(query_id='all'):\n",
        "    queries = list(all_queries.values()) if query_id == 'all' else [all_queries[query_id]]\n",
        "    relevants = all_relevants if query_id == 'all' else {all_queries[query_id]: all_relevants[all_queries[query_id]]}\n",
        "    results = all_results if query_id == 'all' else {all_queries[query_id]: all_results[all_queries[query_id]]}\n",
        "    beta = 1\n",
        "    evaluation_result = 0\n",
        "    for query in queries:\n",
        "        precision = len(set(results[query]) & set(relevants[query])) / len(results[query])\n",
        "        recall = len(set(results[query]) & set(relevants[query])) / len(relevants[query])\n",
        "        if precision > 0 and recall > 0:\n",
        "            evaluation_result += ((beta ** 2 + 1) * precision * recall) / ((beta ** 2) * precision + recall)\n",
        "    evaluation_result /= len(queries)\n",
        "    return evaluation_result\n",
        "\n",
        "\n",
        "print(F_measure())\n",
        "print(F_measure('1'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4251613406466923\n",
            "0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dad4HOuuD2gB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0fd2660a-5f97-4f21-9a34-772dfbbea943"
      },
      "source": [
        "def precision_at_k(query, k, result, relevant):\n",
        "    overlap = [value for value in relevant if value in result]\n",
        "    return len(overlap) / k\n",
        "\n",
        "\n",
        "def average_precision(query):\n",
        "    avg_precision = 0\n",
        "    query_results = all_results[query]\n",
        "    query_relevants = all_relevants[query]\n",
        "    for i in range(len(all_results[query])):\n",
        "        related = 1 if all_results[query][i] in all_relevants[query] else 0\n",
        "        avg_precision += precision_at_k(query, i + 1, query_results[:i + 1], query_relevants[:i + 1]) * related\n",
        "    avg_precision /= len(all_relevants[query])\n",
        "    return avg_precision\n",
        "\n",
        "\n",
        "def MAP(query_id='all'):\n",
        "    queries = list(all_queries.values()) if query_id == 'all' else [all_queries[query_id]]\n",
        "    evaluation_result = 0\n",
        "    for query in queries:\n",
        "        evaluation_result += average_precision(query)\n",
        "    evaluation_result /= len(queries)\n",
        "    return evaluation_result\n",
        "\n",
        "\n",
        "print(MAP())\n",
        "print(MAP('1'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27166566698129874\n",
            "0.5099494255744256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woKuaNhfCHz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ddb2393e-aa83-4cd3-950f-27e5fb822fae"
      },
      "source": [
        "def DCG(result, relevant):\n",
        "    dcg = 1 if result[0] in relevant else 0\n",
        "    for i in range(1, len(result)):\n",
        "        dcg += (1 / log2(i + 1)) if result[i] in relevant else 0\n",
        "    return dcg\n",
        "\n",
        "\n",
        "def NDCG(query_id='all'):\n",
        "    queries = list(all_queries.values()) if query_id == 'all' else [all_queries[query_id]]\n",
        "    relevants = all_relevants if query_id == 'all' else {all_queries[query_id]: all_relevants[all_queries[query_id]]}\n",
        "    results = all_results if query_id == 'all' else {all_queries[query_id]: all_results[all_queries[query_id]]}\n",
        "    evaluation_result = 0\n",
        "    for query in queries:\n",
        "        evaluation_result += DCG(results[query], relevants[query]) / DCG(relevants[query], relevants[query])\n",
        "    evaluation_result /= len(queries)\n",
        "    return evaluation_result\n",
        "\n",
        "\n",
        "print(NDCG())\n",
        "print(NDCG('1'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.505439014468722\n",
            "0.8420009211105769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUs450l5JnfD",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<font color=red size=7>\n",
        "<p></p>\n",
        "<div align=center>نکات پایانی</div>\n",
        "</font>\n",
        "<hr>\n",
        "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
        "<br>\n",
        "۲- فایل‌های \n",
        "ipynb\n",
        "و پایتون \n",
        "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
        "<br>\n",
        "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
        "<br>\n",
        "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
        "<br>\n",
        "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
        "<br>\n",
        "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2AOY8hCJnfE",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\" dir=rtl><font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B titr\" size=30>\n",
        "<p></p>\n",
        "<font color=#FF7500> \n",
        "موفق باشید\n",
        ":)\n",
        "<br>\n",
        "\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    }
  ]
}