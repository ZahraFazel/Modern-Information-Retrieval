{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl><font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "توابع مشترک\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from math import log10\n",
    "\n",
    "\n",
    "def tokenize(docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in docs:\n",
    "        tokenized_doc = {'category': doc['category'] if 'category' in doc.keys() else 0}\n",
    "        all_tokens = []\n",
    "        title_tokens = doc['title'].lower().translate(str.maketrans(' ', ' ', punctuation)).split(' ')\n",
    "        body_tokens = doc['body'].lower().translate(str.maketrans(' ', ' ', punctuation)).split(' ')\n",
    "        for token in title_tokens:\n",
    "            if token != '':\n",
    "                all_tokens.append(token)\n",
    "        for token in body_tokens:\n",
    "            if token != '':\n",
    "                all_tokens.append(token)\n",
    "        tokenized_doc['tokens'] = all_tokens\n",
    "        tokenized_docs.append(tokenized_doc)\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "def stemming(docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_docs = []\n",
    "    for doc in docs:\n",
    "        stemmed_doc = {'category': doc['category']}\n",
    "        all_tokens = []\n",
    "        for token in doc['tokens']:\n",
    "            all_tokens.append(stemmer.stem(token))\n",
    "        stemmed_doc['tokens'] = all_tokens\n",
    "        stemmed_docs.append(stemmed_doc)\n",
    "    return stemmed_docs\n",
    "\n",
    "\n",
    "def lemmatization(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs:\n",
    "        lemmatized_doc = {'category': doc['category']}\n",
    "        all_tokens = []\n",
    "        for token in doc['tokens']:\n",
    "            all_tokens.append(lemmatizer.lemmatize(token))\n",
    "        lemmatized_doc['tokens'] = all_tokens\n",
    "        lemmatized_docs.append(lemmatized_doc)\n",
    "    return lemmatized_docs\n",
    "\n",
    "\n",
    "def stop_words_removal(docs):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_doc = {'category': doc['category']}\n",
    "        all_tokens = []\n",
    "        for token in doc['tokens']:\n",
    "            if token not in stop_words:\n",
    "                all_tokens.append(token)\n",
    "        new_doc['tokens'] = all_tokens\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def tf_idf(docs):\n",
    "    vocabulary = {}\n",
    "    tf = [{} for _ in range(len(docs))]\n",
    "    idf = {}\n",
    "    number_of_terms = 0\n",
    "    for i in range(len(docs)):\n",
    "        for term in docs[i]['tokens']:\n",
    "            if term not in tf[i].keys():\n",
    "                tf[i][term] = 0\n",
    "            tf[i][term] += 1\n",
    "            if term not in vocabulary.keys():\n",
    "                vocabulary[term] = number_of_terms\n",
    "                number_of_terms += 1\n",
    "    for term in vocabulary.keys():\n",
    "        df = 0\n",
    "        for i in range(len(docs)):\n",
    "            if term in tf[i].keys():\n",
    "                df += 1\n",
    "        idf[term] = log10(len(docs) / df) if df != 0 else 0\n",
    "    docs_tf_idf_matrix = [[0 for __ in range(len(vocabulary.keys()))] for _ in range(len(docs))]\n",
    "    for i in range(len(docs)):\n",
    "        for term in tf[i].keys():\n",
    "            docs_tf_idf_matrix[i][vocabulary[term]] = tf[i][term] * idf[term]\n",
    "    return vocabulary, idf, docs_tf_idf_matrix\n",
    "\n",
    "\n",
    "def evaluation(results):\n",
    "    precision = [0, 0, 0, 0, 0]\n",
    "    recall = [0, 0, 0, 0, 0]\n",
    "    fi = [0, 0, 0, 0, 0]\n",
    "    confusion_matrix = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
    "    for result in results:\n",
    "        confusion_matrix[result[0]][result[1]] += 1\n",
    "    for i in range(1, 5):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for result in results:\n",
    "            if result[0] == i and result[1] != i:\n",
    "                fn += 1\n",
    "            if result[0] == i and result[1] == i:\n",
    "                tp += 1\n",
    "            if result[0] != i and result[1] == i:\n",
    "                fp += 1\n",
    "        precision[i] = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "        fi[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i]) if precision[i] + recall[i] != 0 else 0\n",
    "    for i in range(1, 5):\n",
    "        print('\\tCategory {}: \\n\\t\\tPrecision: {:.5f}\\n\\t\\tRecall: {:.5f}'.format(i, precision[i], recall[i]))\n",
    "    print('\\tAccuracy: {:.5f}'.format((confusion_matrix[1][1] + confusion_matrix[2][2] + confusion_matrix[3][3] + confusion_matrix[4][4]) / len(results)))\n",
    "    print('\\tConfusion Matrix: ', end='')\n",
    "    for i in range(1, 5):\n",
    "        print('\\n\\t\\t', end='')\n",
    "        for j in range(1, 5):\n",
    "            print('{: =3}  '.format(confusion_matrix[i][j]), end=\"\")\n",
    "    print('\\n\\tMacro Averaged FI: {:.5f}'.format(sum(fi) / 4))\n",
    "\n",
    "\n",
    "def apply_text_operation(docs, text_operation):\n",
    "    if text_operation == 'stemming':\n",
    "        new_docs = stemming(tokenize(docs))\n",
    "    elif text_operation == 'lemmatization':\n",
    "        new_docs = lemmatization(tokenize(docs))\n",
    "    elif text_operation == 'stop words removal':\n",
    "        new_docs = stop_words_removal(tokenize(docs))\n",
    "    else:\n",
    "        new_docs = tokenize(docs)\n",
    "    return new_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl><font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "لود داده‌ها\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_data_path = 'train.json'\n",
    "test_data_path = 'validation.json'\n",
    "\n",
    "with open(training_data_path) as training_data_file:\n",
    "    training_data = json.loads(training_data_file.read())\n",
    "\n",
    "with open(test_data_path) as test_data_file:\n",
    "    test_data = json.loads(test_data_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "        <br/>\n",
    "KNN\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "vocabulary = {}\n",
    "train_matrix = []\n",
    "category_of_docs = []\n",
    "idf = {}\n",
    "x2 = numpy.asarray([])\n",
    "\n",
    "\n",
    "def KNN_train(docs, text_operation):\n",
    "    new_docs = apply_text_operation(docs, text_operation)\n",
    "    for i in range(len(docs)):\n",
    "        category_of_docs.append(docs[i]['category'])\n",
    "    output = tf_idf(new_docs)\n",
    "    global vocabulary\n",
    "    vocabulary = output[0]\n",
    "    global idf\n",
    "    idf = output[1]\n",
    "    global train_matrix\n",
    "    train_matrix = numpy.asarray(output[2])\n",
    "    global x2\n",
    "    x2 = [row @ row for row in train_matrix]\n",
    "\n",
    "\n",
    "def KNN_classify(doc, text_operation, distance_metric):\n",
    "    tokens = apply_text_operation([doc], text_operation)[0]['tokens']\n",
    "    result = []\n",
    "    test_matrix = [0 for _ in range(len(vocabulary))]\n",
    "    for term in tokens:\n",
    "        if term in vocabulary:\n",
    "            test_matrix[vocabulary[term]] += 1\n",
    "    test_matrix = numpy.asarray([test_matrix[vocabulary[term]] * idf[term] for term in vocabulary.keys()])\n",
    "    if distance_metric == 'euclidean distance':\n",
    "        xy = train_matrix @ test_matrix\n",
    "        y2 = test_matrix @ test_matrix\n",
    "        scores = list(x2 - 2 * xy + y2)\n",
    "        for i in range(5):\n",
    "            result.append(category_of_docs[scores.index(min(scores))])\n",
    "            scores[scores.index(min(scores))] = float('inf')\n",
    "    else:   # distance_metric == 'cosine similarity'\n",
    "        scores = list(numpy.asarray(train_matrix) @ numpy.asarray(test_matrix))\n",
    "        for i in range(5):\n",
    "            result.append(category_of_docs[scores.index(max(scores))])\n",
    "            scores[scores.index(max(scores))] = float('-inf')\n",
    "    return result\n",
    "\n",
    "\n",
    "def KNN_test(docs, mode, distance_metric):\n",
    "    new_data = deepcopy(docs)\n",
    "    results = []\n",
    "    for doc in new_data:\n",
    "        category = doc.pop('category')\n",
    "        first_k_categories = KNN_classify(doc, mode, distance_metric)\n",
    "        results.append((category, first_k_categories))\n",
    "    return results\n",
    "\n",
    "\n",
    "def KNN_print_log(k, distance_metric, all_results, text_operation):\n",
    "    result = []\n",
    "    for i in range(len(all_results)):\n",
    "        result.append((all_results[i][0], numpy.bincount(all_results[i][1][:k]).argmax()))\n",
    "    print('K-Nearest Neighbours:')\n",
    "    print('\\tK: ' + str(k))\n",
    "    print('\\tText Operation: ' + text_operation)\n",
    "    print('\\tDistance Metric: ' + distance_metric)\n",
    "    evaluation(result)\n",
    "\n",
    "\n",
    "def KNN_find_best_hyper_parameter(train_docs, test_docs, parameters, distance_metric):\n",
    "    print(\"Finding best K for K-Nearest Neighbours:\")\n",
    "    KNN_train(train_docs, 'none')\n",
    "    all_results = KNN_test(test_docs, 'none', distance_metric)\n",
    "    for parameter in parameters:\n",
    "        KNN_print_log(parameter, distance_metric, all_results, 'none')\n",
    "\n",
    "\n",
    "def KNN_check_text_operations_effects(train_docs, test_docs, distance_metric):\n",
    "    print(\"Checking effects of text operations on K-Nearest Neighbours:\")\n",
    "    KNN_train(train_docs, 'stemming')\n",
    "    all_results = KNN_test(test_docs, 'stemming', distance_metric)\n",
    "    KNN_print_log(1, distance_metric, all_results, 'stemming')\n",
    "    KNN_train(train_docs, 'lemmatization')\n",
    "    all_results = KNN_test(test_docs, 'lemmatization', distance_metric)\n",
    "    KNN_print_log(1, distance_metric, all_results, 'lemmatization')\n",
    "    KNN_train(train_docs, 'stop words removal')\n",
    "    all_results = KNN_test(test_docs, 'stop words removal', distance_metric)\n",
    "    KNN_print_log(1, distance_metric, all_results, 'stop words removal')\n",
    "\n",
    "\n",
    "def KNN_run(train_docs, test_docs, k, distance_metric, text_operation):\n",
    "    KNN_train(train_docs, 'none')\n",
    "    all_results = KNN_test(test_docs, text_operation, distance_metric)\n",
    "    KNN_print_log(k, distance_metric, all_results, text_operation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- یافتن بهترین مقدار k به ازای معیار فاصله cosine similarity\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بین مقادیر 1، 3 و 5 برای K، بهترین مقدار 5 است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best K for K-Nearest Neighbours:\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: none\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.78030\n",
      "\t\tRecall: 0.80679\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.90423\n",
      "\t\tRecall: 0.85829\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.81143\n",
      "\t\tRecall: 0.74934\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.72180\n",
      "\t\tRecall: 0.79121\n",
      "\tAccuracy: 0.80133\n",
      "\tConfusion Matrix: \n",
      "\t\t309   18   21   35  \n",
      "\t\t 28  321    3   22  \n",
      "\t\t 33    8  284   54  \n",
      "\t\t 26    8   42  288  \n",
      "\tMacro Averaged FI: 0.80201\n",
      "K-Nearest Neighbours:\n",
      "\tK: 3\n",
      "\tText Operation: none\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.79717\n",
      "\t\tRecall: 0.88251\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.90984\n",
      "\t\tRecall: 0.89037\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86280\n",
      "\t\tRecall: 0.74670\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.78010\n",
      "\t\tRecall: 0.81868\n",
      "\tAccuracy: 0.83467\n",
      "\tConfusion Matrix: \n",
      "\t\t338   11   13   21  \n",
      "\t\t 25  333    4   12  \n",
      "\t\t 33   12  283   51  \n",
      "\t\t 28   10   28  298  \n",
      "\tMacro Averaged FI: 0.83429\n",
      "K-Nearest Neighbours:\n",
      "\tK: 5\n",
      "\tText Operation: none\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.83663\n",
      "\t\tRecall: 0.88251\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.91777\n",
      "\t\tRecall: 0.92513\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.84659\n",
      "\t\tRecall: 0.78628\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.80381\n",
      "\t\tRecall: 0.81044\n",
      "\tAccuracy: 0.85133\n",
      "\tConfusion Matrix: \n",
      "\t\t338   11   18   16  \n",
      "\t\t 14  346    3   11  \n",
      "\t\t 29    7  298   45  \n",
      "\t\t 23   13   33  295  \n",
      "\tMacro Averaged FI: 0.85071\n"
     ]
    }
   ],
   "source": [
    "KNN_find_best_hyper_parameter(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)], [1, 3, 5], 'cosine similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- یافتن بهترین مقدار k به ازای معیار فاصله euclidean distance\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بین مقادیر 1، 3 و 5 برای K، بهترین مقدار 5 است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best K for K-Nearest Neighbours:\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: none\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.82609\n",
      "\t\tRecall: 0.34726\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.96154\n",
      "\t\tRecall: 0.26738\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.31873\n",
      "\t\tRecall: 0.90237\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.81481\n",
      "\t\tRecall: 0.36264\n",
      "\tAccuracy: 0.47133\n",
      "\tConfusion Matrix: \n",
      "\t\t133    4  243    3  \n",
      "\t\t 14  100  259    1  \n",
      "\t\t 11    0  342   26  \n",
      "\t\t  3    0  229  132  \n",
      "\tMacro Averaged FI: 0.47009\n",
      "K-Nearest Neighbours:\n",
      "\tK: 3\n",
      "\tText Operation: none\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.43750\n",
      "\t\tRecall: 0.85901\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.88136\n",
      "\t\tRecall: 0.41711\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.75182\n",
      "\t\tRecall: 0.54354\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.65657\n",
      "\t\tRecall: 0.53571\n",
      "\tAccuracy: 0.59067\n",
      "\tConfusion Matrix: \n",
      "\t\t329    6   19   29  \n",
      "\t\t183  156   10   25  \n",
      "\t\t120    5  206   48  \n",
      "\t\t120   10   39  195  \n",
      "\tMacro Averaged FI: 0.59173\n",
      "K-Nearest Neighbours:\n",
      "\tK: 5\n",
      "\tText Operation: none\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.68069\n",
      "\t\tRecall: 0.71802\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.94156\n",
      "\t\tRecall: 0.38770\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.66954\n",
      "\t\tRecall: 0.61478\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.45118\n",
      "\t\tRecall: 0.73626\n",
      "\tAccuracy: 0.61400\n",
      "\tConfusion Matrix: \n",
      "\t\t275    5   24   79  \n",
      "\t\t 45  145   36  148  \n",
      "\t\t 44    3  233   99  \n",
      "\t\t 40    1   55  268  \n",
      "\tMacro Averaged FI: 0.61215\n"
     ]
    }
   ],
   "source": [
    "KNN_find_best_hyper_parameter(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)], [1, 3, 5], 'euclidean distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- بررسی تأثیر روش‌های پردازش متن بر روی دسته‌بندی به ازای معیار فاصله cosine similarity\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بهترین اثر را stop words removal و بدترین اثر را stemming دارد.    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking effects of text operations on K-Nearest Neighbours:\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: stemming\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.78880\n",
      "\t\tRecall: 0.80940\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.92486\n",
      "\t\tRecall: 0.85561\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.79887\n",
      "\t\tRecall: 0.74406\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.71569\n",
      "\t\tRecall: 0.80220\n",
      "\tAccuracy: 0.80267\n",
      "\tConfusion Matrix: \n",
      "\t\t310   12   23   38  \n",
      "\t\t 30  320    6   18  \n",
      "\t\t 29    8  282   60  \n",
      "\t\t 24    6   42  292  \n",
      "\tMacro Averaged FI: 0.80371\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: lemmatization\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.78987\n",
      "\t\tRecall: 0.81462\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.92090\n",
      "\t\tRecall: 0.87166\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.80000\n",
      "\t\tRecall: 0.73879\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.71820\n",
      "\t\tRecall: 0.79121\n",
      "\tAccuracy: 0.80400\n",
      "\tConfusion Matrix: \n",
      "\t\t312   12   22   37  \n",
      "\t\t 26  326    4   18  \n",
      "\t\t 32    9  280   58  \n",
      "\t\t 25    7   44  288  \n",
      "\tMacro Averaged FI: 0.80469\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: stop words removal\n",
      "\tDistance Metric: cosine similarity\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.78554\n",
      "\t\tRecall: 0.82245\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.90503\n",
      "\t\tRecall: 0.86631\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.80563\n",
      "\t\tRecall: 0.75462\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.74352\n",
      "\t\tRecall: 0.78846\n",
      "\tAccuracy: 0.80800\n",
      "\tConfusion Matrix: \n",
      "\t\t315   18   19   31  \n",
      "\t\t 29  324    5   16  \n",
      "\t\t 33    8  286   52  \n",
      "\t\t 24    8   45  287  \n",
      "\tMacro Averaged FI: 0.80836\n"
     ]
    }
   ],
   "source": [
    "KNN_check_text_operations_effects(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)], 'cosine similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- بررسی تأثیر روش‌های پردازش متن بر روی دسته‌بندی به ازای معیار فاصله euclidean distance\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بدترین اثر را stop words removal و بهترین اثر را stemming دارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking effects of text operations on K-Nearest Neighbours:\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: stemming\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.84167\n",
      "\t\tRecall: 0.52742\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.92903\n",
      "\t\tRecall: 0.38503\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.38318\n",
      "\t\tRecall: 0.86544\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.77912\n",
      "\t\tRecall: 0.53297\n",
      "\tAccuracy: 0.57867\n",
      "\tConfusion Matrix: \n",
      "\t\t202    8  162   11  \n",
      "\t\t 16  144  206    8  \n",
      "\t\t 14    1  328   36  \n",
      "\t\t  8    2  160  194  \n",
      "\tMacro Averaged FI: 0.58926\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: lemmatization\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.83920\n",
      "\t\tRecall: 0.43603\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.96581\n",
      "\t\tRecall: 0.30214\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.34295\n",
      "\t\tRecall: 0.87863\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.77934\n",
      "\t\tRecall: 0.45604\n",
      "\tAccuracy: 0.51933\n",
      "\tConfusion Matrix: \n",
      "\t\t167    4  200   12  \n",
      "\t\t 16  113  243    2  \n",
      "\t\t 13    0  333   33  \n",
      "\t\t  3    0  195  166  \n",
      "\tMacro Averaged FI: 0.52572\n",
      "K-Nearest Neighbours:\n",
      "\tK: 1\n",
      "\tText Operation: stop words removal\n",
      "\tDistance Metric: euclidean distance\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.78414\n",
      "\t\tRecall: 0.46475\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.95041\n",
      "\t\tRecall: 0.30749\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.34792\n",
      "\t\tRecall: 0.88127\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.76042\n",
      "\t\tRecall: 0.40110\n",
      "\tAccuracy: 0.51533\n",
      "\tConfusion Matrix: \n",
      "\t\t178    5  191    9  \n",
      "\t\t 21  115  230    8  \n",
      "\t\t 16    0  334   29  \n",
      "\t\t 12    1  205  146  \n",
      "\tMacro Averaged FI: 0.51808\n"
     ]
    }
   ],
   "source": [
    "KNN_check_text_operations_effects(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)], 'euclidean distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "        <br/>\n",
    "Naïve Bayes\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "vocabulary = set()\n",
    "probability_of_categories = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "probability_of_terms = {}\n",
    "\n",
    "\n",
    "def NB_preprocess(docs, text_operation):\n",
    "    new_docs = apply_text_operation(docs, text_operation)\n",
    "    terms_of_category = {1: {}, 2: {}, 3: {}, 4: {}}\n",
    "    for doc in new_docs:\n",
    "        for word in doc['tokens']:\n",
    "            if word not in terms_of_category[doc['category']]:\n",
    "                terms_of_category[doc['category']][word] = 0\n",
    "            terms_of_category[doc['category']][word] += 1\n",
    "            vocabulary.add(word)\n",
    "    return terms_of_category\n",
    "\n",
    "\n",
    "def NB_train(docs, alpha, text_operation):\n",
    "    terms_of_category = NB_preprocess(docs, text_operation)\n",
    "    number_of_terms = {}\n",
    "    for doc in docs:\n",
    "        probability_of_categories[doc['category']] += 1\n",
    "    for category in probability_of_categories.keys():\n",
    "        probability_of_categories[category] /= len(docs)\n",
    "    for term in vocabulary:\n",
    "        number_of_terms[term] = {category: 0 for category in probability_of_categories.keys()}\n",
    "        for category in probability_of_categories.keys():\n",
    "            if term in terms_of_category[category].keys():\n",
    "                number_of_terms[term][category] = terms_of_category[category][term]\n",
    "    for term in vocabulary:\n",
    "        probability_of_terms[term] = {category: 0.0 for category in probability_of_categories.keys()}\n",
    "        for category in probability_of_categories.keys():\n",
    "            probability_of_terms[term][category] = (number_of_terms[term][category] + alpha) / (len(terms_of_category[category]) + alpha * len(vocabulary))\n",
    "\n",
    "\n",
    "def NB_classify(doc, text_operation):\n",
    "    tokens = apply_text_operation([doc], text_operation)[0]['tokens']\n",
    "    scores = {}\n",
    "    for category in probability_of_categories.keys():\n",
    "        scores[category] = log10(probability_of_categories[category])\n",
    "        for term in tokens:\n",
    "            if term in vocabulary:\n",
    "                scores[category] += log10(probability_of_terms[term][category])\n",
    "    for category in scores.keys():\n",
    "        if scores[category] == max(scores.values()):\n",
    "            return category\n",
    "\n",
    "\n",
    "def NB_test(docs, alpha, text_operation):\n",
    "    new_data = deepcopy(docs)\n",
    "    results = []\n",
    "    for doc in new_data:\n",
    "        category = doc.pop('category')\n",
    "        predicted_category = NB_classify(doc, text_operation)\n",
    "        results.append((category, predicted_category))\n",
    "    print('Naive Bayes:')\n",
    "    print('\\tAlpha: ' + str(alpha))\n",
    "    print('\\tText Operation: ' + text_operation)\n",
    "    evaluation(results)\n",
    "\n",
    "\n",
    "def NB_find_best_hyper_parameter(train_docs, test_docs, parameters, text_operation):\n",
    "    print(\"Finding best alpha for Naive Bayes:\")\n",
    "    NB_train(train_docs, parameters[0], text_operation)\n",
    "    NB_test(test_docs, parameters[0], text_operation)\n",
    "    NB_train(train_docs, parameters[1], text_operation)\n",
    "    NB_test(test_docs, parameters[1], text_operation)\n",
    "    NB_train(train_docs, parameters[2], text_operation)\n",
    "    NB_test(test_docs, parameters[2], text_operation)\n",
    "\n",
    "\n",
    "def NB_check_text_operations_effects(train_docs, test_docs, alpha):\n",
    "    print(\"Checking effects of text operations on Naive Bayes:\")\n",
    "    NB_train(train_docs, alpha, 'stemming')\n",
    "    NB_test(test_docs, alpha, 'stemming')\n",
    "    NB_train(train_docs, alpha, 'lemmatization')\n",
    "    NB_test(test_docs, alpha, 'lemmatization')\n",
    "    NB_train(train_docs, alpha, 'stop words removal')\n",
    "    NB_test(test_docs, alpha, 'stop words removal')\n",
    "\n",
    "\n",
    "def NB_run(train_docs, test_docs, alpha, text_operation):\n",
    "    NB_train(train_docs, alpha, text_operation)\n",
    "    NB_test(test_docs, alpha, text_operation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- یافتن بهترین مقدار $\\alpha$\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بین مقادیر 0.25، 1 و 4 برای $\\alpha$، بهترین مقدار 1 است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best alpha for Naive Bayes:\n",
      "Naive Bayes:\n",
      "\tAlpha: 0.25\n",
      "\tText Operation: none\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.88816\n",
      "\t\tRecall: 0.90000\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.94080\n",
      "\t\tRecall: 0.97467\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.85229\n",
      "\t\tRecall: 0.86933\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.89112\n",
      "\t\tRecall: 0.82933\n",
      "\tAccuracy: 0.89333\n",
      "\tConfusion Matrix: \n",
      "\t\t675   27   32   16  \n",
      "\t\t 13  731    3    3  \n",
      "\t\t 33    8  652   57  \n",
      "\t\t 39   11   78  622  \n",
      "\tMacro Averaged FI: 0.89283\n",
      "Naive Bayes:\n",
      "\tAlpha: 1\n",
      "\tText Operation: none\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.89376\n",
      "\t\tRecall: 0.89733\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.93967\n",
      "\t\tRecall: 0.97600\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86479\n",
      "\t\tRecall: 0.86133\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.87656\n",
      "\t\tRecall: 0.84267\n",
      "\tAccuracy: 0.89433\n",
      "\tConfusion Matrix: \n",
      "\t\t673   29   29   19  \n",
      "\t\t 11  732    2    5  \n",
      "\t\t 31    8  646   65  \n",
      "\t\t 38   10   70  632  \n",
      "\tMacro Averaged FI: 0.89384\n",
      "Naive Bayes:\n",
      "\tAlpha: 4\n",
      "\tText Operation: none\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.88992\n",
      "\t\tRecall: 0.89467\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.93011\n",
      "\t\tRecall: 0.97600\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.87295\n",
      "\t\tRecall: 0.85200\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.86520\n",
      "\t\tRecall: 0.83867\n",
      "\tAccuracy: 0.89033\n",
      "\tConfusion Matrix: \n",
      "\t\t671   30   27   22  \n",
      "\t\t 11  732    2    5  \n",
      "\t\t 30   10  639   71  \n",
      "\t\t 42   15   64  629  \n",
      "\tMacro Averaged FI: 0.88972\n"
     ]
    }
   ],
   "source": [
    "NB_find_best_hyper_parameter(training_data, test_data, [0.25, 1, 4], 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- بررسی تأثیر روش‌های پردازش متن بر روی دسته‌بندی\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بهترین اثر را stop words removal و بدترین اثر را lemmatization دارد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking effects of text operations on Naive Bayes:\n",
      "Naive Bayes:\n",
      "\tAlpha: 1\n",
      "\tText Operation: stemming\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.89093\n",
      "\t\tRecall: 0.90400\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.94201\n",
      "\t\tRecall: 0.97467\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86821\n",
      "\t\tRecall: 0.85200\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.87345\n",
      "\t\tRecall: 0.84667\n",
      "\tAccuracy: 0.89433\n",
      "\tConfusion Matrix: \n",
      "\t\t678   26   28   18  \n",
      "\t\t 11  731    3    5  \n",
      "\t\t 32   10  639   69  \n",
      "\t\t 40    9   66  635  \n",
      "\tMacro Averaged FI: 0.89384\n",
      "Naive Bayes:\n",
      "\tAlpha: 1\n",
      "\tText Operation: lemmatization\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.89196\n",
      "\t\tRecall: 0.90267\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.93726\n",
      "\t\tRecall: 0.97600\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86957\n",
      "\t\tRecall: 0.85333\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.87569\n",
      "\t\tRecall: 0.84533\n",
      "\tAccuracy: 0.89433\n",
      "\tConfusion Matrix: \n",
      "\t\t677   27   29   17  \n",
      "\t\t 12  732    1    5  \n",
      "\t\t 32   10  640   68  \n",
      "\t\t 38   12   66  634  \n",
      "\tMacro Averaged FI: 0.89378\n",
      "Naive Bayes:\n",
      "\tAlpha: 1\n",
      "\tText Operation: stop words removal\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.89933\n",
      "\t\tRecall: 0.89333\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.94466\n",
      "\t\tRecall: 0.97867\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86093\n",
      "\t\tRecall: 0.86667\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.87552\n",
      "\t\tRecall: 0.84400\n",
      "\tAccuracy: 0.89567\n",
      "\tConfusion Matrix: \n",
      "\t\t670   26   33   21  \n",
      "\t\t 10  734    1    5  \n",
      "\t\t 29    7  650   64  \n",
      "\t\t 36   10   71  633  \n",
      "\tMacro Averaged FI: 0.89524\n"
     ]
    }
   ],
   "source": [
    "NB_check_text_operations_effects(training_data, test_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "        <br/>\n",
    "SVM\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from copy import deepcopy\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "vocabulary = {}\n",
    "SVM = None\n",
    "idf = {}\n",
    "\n",
    "\n",
    "def SVM_train(docs, c):\n",
    "    new_docs = tokenize(docs)\n",
    "    output = tf_idf(new_docs)\n",
    "    global vocabulary\n",
    "    vocabulary = output[0]\n",
    "    global idf\n",
    "    idf = output[1]\n",
    "    train_x = scipy.sparse.csr_matrix(output[2])\n",
    "    train_y = [new_docs[i]['category'] for i in range(len(new_docs))]\n",
    "    global SVM\n",
    "    SVM = svm.SVC(C=c, kernel='linear', degree=3, gamma='auto', max_iter=12000)\n",
    "    SVM.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "def SVM_classify(doc):\n",
    "    tokens = tokenize([doc])[0]['tokens']\n",
    "    test_x = [0 for _ in range(len(vocabulary))]\n",
    "    for term in tokens:\n",
    "        if term in vocabulary:\n",
    "            test_x[vocabulary[term]] += 1\n",
    "    test_x = scipy.sparse.csr_matrix([test_x[vocabulary[term]] * idf[term] for term in vocabulary.keys()])\n",
    "    return SVM.predict(test_x)[0]\n",
    "\n",
    "\n",
    "def SVM_test(docs, c):\n",
    "    new_data = deepcopy(docs)\n",
    "    results = []\n",
    "    for doc in new_data:\n",
    "        category = doc.pop('category')\n",
    "        predicted_category = SVM_classify(doc)\n",
    "        results.append((category, predicted_category))\n",
    "    print('SVM:')\n",
    "    print('\\tC: ' + str(c))\n",
    "    evaluation(results)\n",
    "\n",
    "\n",
    "def SVM_find_best_hyper_parameter(train_docs, test_docs, parameters):\n",
    "    print(\"Finding best C for SVM:\")\n",
    "    SVM_train(train_docs, parameters[0])\n",
    "    SVM_test(test_docs, parameters[0])\n",
    "    SVM_train(train_docs, parameters[1])\n",
    "    SVM_test(test_docs, parameters[1])\n",
    "    SVM_train(train_docs, parameters[2])\n",
    "    SVM_test(test_docs, parameters[2])\n",
    "\n",
    "\n",
    "def SVM_run(train_docs, test_docs, c):\n",
    "    SVM_train(train_docs, c)\n",
    "    SVM_test(test_docs, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- یافتن بهترین مقدار C\n",
    "    </font>\n",
    "        <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بین مقادیر 0.001، 0.01 و 0.1 برای C، بهترین مقدار 0.01 است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best C for SVM:\n",
      "SVM:\n",
      "\tC: 0.001\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.92898\n",
      "\t\tRecall: 0.85379\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.92658\n",
      "\t\tRecall: 0.97861\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.87714\n",
      "\t\tRecall: 0.81003\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.78908\n",
      "\t\tRecall: 0.87363\n",
      "\tAccuracy: 0.87867\n",
      "\tConfusion Matrix: \n",
      "\t\t327   15   16   25  \n",
      "\t\t  3  366    1    4  \n",
      "\t\t  9    7  307   56  \n",
      "\t\t 13    7   26  318  \n",
      "\tMacro Averaged FI: 0.87828\n",
      "SVM:\n",
      "\tC: 0.01\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.89946\n",
      "\t\tRecall: 0.86423\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.94010\n",
      "\t\tRecall: 0.96524\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.87397\n",
      "\t\tRecall: 0.84169\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.84073\n",
      "\t\tRecall: 0.88462\n",
      "\tAccuracy: 0.88867\n",
      "\tConfusion Matrix: \n",
      "\t\t331   14   19   19  \n",
      "\t\t  7  361    2    4  \n",
      "\t\t 15    7  319   38  \n",
      "\t\t 15    2   25  322  \n",
      "\tMacro Averaged FI: 0.88841\n",
      "SVM:\n",
      "\tC: 0.1\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.87500\n",
      "\t\tRecall: 0.85901\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.93211\n",
      "\t\tRecall: 0.95455\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.86464\n",
      "\t\tRecall: 0.82586\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.83113\n",
      "\t\tRecall: 0.86538\n",
      "\tAccuracy: 0.87600\n",
      "\tConfusion Matrix: \n",
      "\t\t329   14   19   21  \n",
      "\t\t 11  357    1    5  \n",
      "\t\t 20    8  313   38  \n",
      "\t\t 16    4   29  315  \n",
      "\tMacro Averaged FI: 0.87571\n"
     ]
    }
   ],
   "source": [
    "SVM_find_best_hyper_parameter(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)], [0.001, 0.01, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "        <br/>\n",
    "Random Forest\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import deepcopy\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "vocabulary = {}\n",
    "random_forest_classifier = None\n",
    "idf = {}\n",
    "\n",
    "\n",
    "def RF_train(docs, number_of_trees, max_depth):\n",
    "    output = tf_idf(tokenize(docs))\n",
    "    global vocabulary\n",
    "    vocabulary = output[0]\n",
    "    global idf\n",
    "    idf = output[1]\n",
    "    train_x = scipy.sparse.csc_matrix(output[2])\n",
    "    train_y = [docs[i]['category'] for i in range(len(docs))]\n",
    "    global random_forest_classifier\n",
    "    random_forest_classifier = RandomForestClassifier(n_estimators=number_of_trees, max_depth=max_depth)\n",
    "    random_forest_classifier.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "def RF_classify(doc):\n",
    "    tokens = tokenize([doc])[0]['tokens']\n",
    "    test_x = [0 for _ in range(len(vocabulary))]\n",
    "    for term in tokens:\n",
    "        if term in vocabulary:\n",
    "            test_x[vocabulary[term]] += 1\n",
    "    test_x = scipy.sparse.csc_matrix([test_x[vocabulary[term]] * idf[term] for term in vocabulary.keys()])\n",
    "    return random_forest_classifier.predict(test_x)[0]\n",
    "\n",
    "\n",
    "def RF_test(docs, number_of_trees, max_depth):\n",
    "    new_data = deepcopy(docs)\n",
    "    results = []\n",
    "    for doc in new_data:\n",
    "        category = doc.pop('category')\n",
    "        predicted_category = RF_classify(doc)\n",
    "        results.append((category, predicted_category))\n",
    "    print('Random Forest:')\n",
    "    print('\\tNumber of trees: ' + str(number_of_trees))\n",
    "    print('\\tMaximum depth: ' + str(max_depth))\n",
    "    evaluation(results)\n",
    "\n",
    "\n",
    "def RF_find_best_hyper_parameter(train_docs, test_docs, parameters):\n",
    "    print(\"Finding best number of trees & depth for Random Forest:\")\n",
    "    RF_train(train_docs, parameters['n_trees'][0], parameters['max_depth'][0])\n",
    "    RF_test(test_docs, parameters['n_trees'][0], parameters['max_depth'][0])\n",
    "    RF_train(train_docs, parameters['n_trees'][1], parameters['max_depth'][1])\n",
    "    RF_test(test_docs, parameters['n_trees'][1], parameters['max_depth'][1])\n",
    "    RF_train(train_docs, parameters['n_trees'][2], parameters['max_depth'][2])\n",
    "    RF_test(test_docs, parameters['n_trees'][2], parameters['max_depth'][2])\n",
    "\n",
    "\n",
    "def RF_run(train_docs, test_docs, number_of_trees, max_depth):\n",
    "    RF_train(train_docs, number_of_trees, max_depth)\n",
    "    RF_test(test_docs, number_of_trees, max_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=4 color=\"#639a67\" >\n",
    "- یافتن بهترین مقادیر برای تعداد درخت‌ها و عمق‌ها\n",
    "    </font>\n",
    "        <br/>\n",
    "    <font face=\"B Mitra\" size=1>\n",
    "* از نصف داده‌ها استفاده‌شده‌است.\n",
    "    </font>\n",
    "    <br/>\n",
    "    <font face=\"B Mitra\" size=3>\n",
    "با توجه به نتایج زیر، بین مقادیر 25، 50 و 100 برای تعداد درخت‌ها، بهترین مقدار 100 و بین مقادیر 5000، 10000 و 20000 برای عمق درخت‌ها، بهترین مقدار 20000 است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best number of trees & depth for Random Forest:\n",
      "Random Forest:\n",
      "\tNumber of trees: 25\n",
      "\tMaximum depth: 5000\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.85154\n",
      "\t\tRecall: 0.79373\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.81412\n",
      "\t\tRecall: 0.92513\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.80290\n",
      "\t\tRecall: 0.73087\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.74263\n",
      "\t\tRecall: 0.76099\n",
      "\tAccuracy: 0.80267\n",
      "\tConfusion Matrix: \n",
      "\t\t304   29   23   27  \n",
      "\t\t 10  346    5   13  \n",
      "\t\t 23   23  277   56  \n",
      "\t\t 20   27   40  277  \n",
      "\tMacro Averaged FI: 0.80115\n",
      "Random Forest:\n",
      "\tNumber of trees: 50\n",
      "\tMaximum depth: 10000\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.86501\n",
      "\t\tRecall: 0.81984\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.84524\n",
      "\t\tRecall: 0.94920\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.83237\n",
      "\t\tRecall: 0.75989\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.74933\n",
      "\t\tRecall: 0.76374\n",
      "\tAccuracy: 0.82333\n",
      "\tConfusion Matrix: \n",
      "\t\t314   24   15   30  \n",
      "\t\t  5  355    3   11  \n",
      "\t\t 22   17  288   52  \n",
      "\t\t 22   24   40  278  \n",
      "\tMacro Averaged FI: 0.82174\n",
      "Random Forest:\n",
      "\tNumber of trees: 100\n",
      "\tMaximum depth: 20000\n",
      "\tCategory 1: \n",
      "\t\tPrecision: 0.87955\n",
      "\t\tRecall: 0.81984\n",
      "\tCategory 2: \n",
      "\t\tPrecision: 0.85372\n",
      "\t\tRecall: 0.95187\n",
      "\tCategory 3: \n",
      "\t\tPrecision: 0.82709\n",
      "\t\tRecall: 0.75726\n",
      "\tCategory 4: \n",
      "\t\tPrecision: 0.75989\n",
      "\t\tRecall: 0.79121\n",
      "\tAccuracy: 0.83000\n",
      "\tConfusion Matrix: \n",
      "\t\t314   24   19   26  \n",
      "\t\t  4  356    3   11  \n",
      "\t\t 20   18  287   54  \n",
      "\t\t 19   19   38  288  \n",
      "\tMacro Averaged FI: 0.82866\n"
     ]
    }
   ],
   "source": [
    "RF_find_best_hyper_parameter(training_data[:int(len(training_data) / 2)], test_data[:int(len(test_data) / 2)],\n",
    "                                       {'n_trees': [25, 50, 100], 'max_depth': [5000, 10000, 20000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;line-height:200%;text-align:justify\" dir=rtl>\n",
    "    <font face=\"B Mitra\" size=6 color=\"#2b580c\" >\n",
    "        <br/>\n",
    "        KMeans\n",
    "            </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def kmeans_run(docs):\n",
    "    new_docs = tokenize(docs)\n",
    "    vocabulary, idf, train_matrix = tf_idf(new_docs)\n",
    "    categories = [new_docs[i]['category'] for i in range(len(new_docs))]\n",
    "    centroids = [numpy.random.random(len(vocabulary)) for _ in range(4)]\n",
    "    clusters = [[] for _ in range(4)]\n",
    "    train_matrix = numpy.asarray(train_matrix)\n",
    "    temp = [row @ row for row in train_matrix]\n",
    "    x2 = numpy.asarray([[temp[i] for _ in range(4)] for i in range(len(docs))])\n",
    "    for i in range(100):\n",
    "        for j in range(4):\n",
    "            clusters[j].clear()\n",
    "        xy = train_matrix @ numpy.transpose(centroids)\n",
    "        temp = [row @ row for row in numpy.asarray(centroids)]\n",
    "        y2 = numpy.asarray([temp for _ in range(len(docs))])\n",
    "        distance = list(x2 - 2 * xy + y2)\n",
    "        for j in range(len(new_docs)):\n",
    "            clusters[list(distance[j]).index(min(list(distance[j])))].append(j)\n",
    "        for j in range(4):\n",
    "            sum_of_vectors = [0 for _ in range(len(vocabulary))]\n",
    "            for k in clusters[j]:\n",
    "                sum_of_vectors = numpy.add(sum_of_vectors, train_matrix[k])\n",
    "            centroids[j] = numpy.asarray(sum_of_vectors / len(clusters[j]) if len(clusters[j]) > 0 else sum_of_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_run(training_data[:int(len(training_data) / 2)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
